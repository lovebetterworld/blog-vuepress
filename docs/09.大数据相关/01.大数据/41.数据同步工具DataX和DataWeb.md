---
title: 数据同步工具DataX和DataWeb
date: 2022-10-26 09:38:36
permalink: /bigdata/DataX01/
categories: 
  - DataX
tags: 
  - DataX
---

- [大数据Hadoop之——数据同步工具DataX - 掘金 (juejin.cn)](https://juejin.cn/post/7100939690898882573#heading-10)

## 一、概述

> DataX 是阿里云 [DataWorks数据集成](https://link.juejin.cn?target=https%3A%2F%2Fwww.aliyun.com%2Fproduct%2Fbigdata%2Fide) 的开源版本，在阿里巴巴集团内被广泛使用的离线数据同步工具/平台。DataX 实现了包括 MySQL、Oracle、OceanBase、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、Hologres、DRDS 等各种异构数据源之间高效的数据同步功能。

Gitee：[github.com/alibaba/Dat…](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Falibaba%2FDataX)

GitHub地址：[github.com/alibaba/Dat…](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Falibaba%2FDataX)

文档：[github.com/alibaba/Dat…](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Falibaba%2FDataX%2Fblob%2Fmaster%2Fintroduction.md)

> DataX 是一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b79655df94f24f6cac341f34431f3753~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

- 为了解决异构数据源同步问题，DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。
- DataX在阿里巴巴集团内被广泛使用，承担了所有大数据的离线同步业务，并已持续稳定运行了6年之久。目前每天完成同步8w多道作业，每日传输数据量超过300TB。

## 二、DataX3.0框架设计

> DataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f1ca3340b8704e3eb8adfdfc7c036e02~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

- **Reader**：Reader为数据采集模块，**负责采集数据源的数据**，将数据发送给Framework。
- **Writer**： Writer为数据写入模块，**负责不断向Framework取数据**，并将数据写入到目的端。
- **Framework**：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。

## 三、DataX3.0架构

> DataX 3.0 开源版本支持单机多线程模式完成同步作业运行，本小节按一个DataX作业生命周期的时序图，从整体架构设计非常简要说明DataX各个模块相互关系。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f90f02c880c942f187a5aa47390b1f0b~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

### 3.1 核心模块介绍

- **DataX完成单个数据同步的作业，我们称之为Job**，DataX接受到一个Job之后，将启动一个进程来完成整个作业同步过程。DataX Job模块是单个作业的中枢管理节点，承担了数据清理、子任务切分(将单一作业计算转化为多个子Task)、TaskGroup管理等功能。
- DataXJob启动后，会根据不同的源端切分策略，将Job切分成多个小的Task(子任务)，以便于并发执行。**Task便是DataX作业的最小单元**，每一个Task都会负责一部分数据的同步工作。
- 切分多个Task之后，DataX Job会调用Scheduler模块，根据配置的并发数据量，将拆分成的Task重新组合，组装成TaskGroup(任务组)。每一个TaskGroup负责以一定的并发运行完毕分配好的所有Task，**默认单个任务组的并发数量为5**。
- 每一个Task都由TaskGroup负责启动，Task启动后，会固定启动`Reader—>Channel—>Writer`的线程来完成任务同步工作。
- DataX作业运行起来之后， Job监控并等待多个TaskGroup模块任务完成，等待所有TaskGroup任务完成后Job成功退出。否则，异常退出，进程退出值非0

### 3.2 DataX调度流程

举例来说，用户提交了一个DataX作业，并且配置了20个并发，目的是将一个100张分表的mysql数据同步到`odps（Open Data Processing Service：开发数据处理服务）`里面。 DataX的调度决策思路是：

- DataXJob根据分库分表切分成了100个Task。
- 根据20个并发，DataX计算共需要分配4个TaskGroup。
- 4个TaskGroup平分切分好的100个Task，每一个TaskGroup负责以5个并发共计运行25个Task。

## 四、环境部署

### 4.1 下载

```bash
$ mkdir -p /opt/datax ; cd /opt/datax
$ wget http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz
```

### 4.2 设置环境变量

```bash
$ cd /opt/datax
$ vi /etc/profile
export DATAX_HOME=/opt/datax
export PATH=$DATAX_HOME/bin:$PATH
$ source /etc/profile
```

当自测DataX运行出错时，删除一些临时文件。

```bash
rm -fr /opt/datax/plugin/*/._*
```

## 五、DataX 实战示例

DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入。DataX目前支持数据如下图，详情请查看[GitHub官方文档](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Falibaba%2FDataX)：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6751d48bb90e425d9570bb4492f116e6~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

### 4.1 MYSQL to HDFS

#### 4.1.1 准备好库表数据

```bash
$ mysql -uroot -p
密码：123456

creta database datax;

CREATE TABLE IF NOT EXISTS `datax`.`person` (
 `id` int(10) NOT NULL AUTO_INCREMENT COMMENT 'ID',
 `name` VARCHAR(32) COMMENT '用户名',
 `age` int(10) COMMENT '年龄',
 PRIMARY KEY (`id`)
)ENGINE=INNODB DEFAULT CHARSET=utf8;

insert into person(name,age) values ('person001',18) ,('person002',19),('person003',20),('person004',21),('person005',22);

select * from datax.person;
```

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/96942d5c22924a37bac6593e6d28b088~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

#### 4.1.2 配置json文件

```bash
$ cd $DATAX_HOME
$ mkdir test
$ cat > ./test/mysql2hdfs <<EOF
{
    "job": {
        "setting": {
            "speed": {
                 "channel":1
            }
        },
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "username": "root",
                        "password": "123456",
                        "connection": [
                            {
                                "querySql": [
                                    "select * from datax.person;"
                                ],
                                "jdbcUrl": [
                                    "jdbc:mysql://hadoop-node1:3306/datax?characterEncoding=utf8&useSSL=false&serverTimezone=UTC&rewriteBatchedStatements=true"
                                ]
                            }
                        ]
                    }
                },
                "writer": {
                    "name": "streamwriter",
                    "parameter": {
                        "defaultFS": "hdfs://hadoop-node1:8082",
                        "fileType": "text",
                        "path": "/tmp/datax/",
                        "fileName": "person",
                        "column": [
                            {
                                "name": "id",
                                "type": "INT"
                            },
                            {
                                "name": "name",
                                "type": "STRING"
                            },
                            {
                                "name": "age",
                                "type": "INT"
                            }
                        ],
                        "writeMode": "append",
                        "fieldDelimiter": ","
                    }
                }
            }
        ]
    }
}
EOF

$ hadoop fs -mkdir /tmp/datax/
```

#### 4.1.3 执行

```bash
$ cd $DATAX_HOME
$ python2 bin/datax.py test/mysql2hdfs
```

> 【温馨提示】如果mysql连接不上，请更换对应版本的mysql驱动，`$DATA_HOME/plugin/reader/mysqlreader/libs/mysql-connector-java-*`

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3907fb8a13554536838a12d58316093e~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

#### 4.1.4 验证

打开HDFS web检查 

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/57d0d3a697164b6d934a923382181799~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

### 4.2 MYSQL to Hive

- [DataX之MySQL数据写入Hive](https://blog.csdn.net/docsz/article/details/116303979)
- [使用DataX采集Mysql数据到Hive](https://blog.csdn.net/pblh123/article/details/126990820)
- [dataX同步mysql至hive](https://blog.csdn.net/AyubLIbra/article/details/115838882)

#### 4.2.1 准备好hive库表数据

```bash
$ beeline -u jdbc:hive2://hadoop-node1:11000  -n root

-- 创建库
CREATE DATABASE datax

-- 创建表时指定库，指定分隔符
CREATE TABLE  IF NOT EXISTS datax.hive_person (
id INT COMMENT 'ID',
name STRING COMMENT '名字',
age INT COMMENT '年龄'
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';
```

#### 4.2.2 配置json文件

> 【温馨提示】其实这里也是推送数据HDFS文件，只不过时推送到表目录下。只需要将上面的json配置改一行就行了。完整配置如下：

```bash
{
    "job": {
        "setting": {
            "speed": {
                 "channel":1
            }
        },
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "username": "root",
                        "password": "123456",
                        "connection": [
                            {
                                "querySql": [
                                    "select * from datax.person;"
                                ],
                                "jdbcUrl": [
                                    "jdbc:mysql://hadoop-node1:3306/datax?characterEncoding=utf8&useSSL=false&serverTimezone=UTC&rewriteBatchedStatements=true"
                                ]
                            }
                        ]
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "defaultFS": "hdfs://hadoop-node1:8082",
                        "fileType": "text",
                        "path": "/user/hive/warehouse/datax.db/hive_person",
                        "fileName": "person",
                        "column": [
                            {
                                "name": "id",
                                "type": "INT"
                            },
                            {
                                "name": "name",
                                "type": "STRING"
                            },
                            {
                                "name": "age",
                                "type": "INT"
                            }
                        ],
                        "writeMode": "append",
                        "fieldDelimiter": ","
                    }
                }
            }
        ]
    }
}
```

JSON模板2：

```json
{
    "job": {
        "setting": {
            "speed": {
                 "channel": 3
            },
            "errorLimit": {
                "record": 0,
                "percentage": 0.02
            }
        },
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "username": "用户名",
                        "password": "密码",
                        "column": [
				"deptno",
				"dname",
				"loc"
                        ],
                        "connection": [
                            {
                                "table": [
                                    "dept"
                                ],
                                "jdbcUrl": [
					"jdbc:mysql://IP:3306/test"
                                ]
                            }
                        ]
                    }
                },
               "writer": {
                    "name": "hdfswriter",
                    "parameter": {
			"defaultFS": "hdfs://hdfs-ha",
		    "hadoopConfig":{
			"dfs.nameservices": "hdfs-ha",
			"dfs.ha.namenodes.hdfs-ha": "nn1,nn2",
			"dfs.namenode.rpc-address.hdfs-ha.nn1": "node01:8020",
			"dfs.namenode.rpc-address.hdfs-ha.nn2": "node02:8020",
			"dfs.client.failover.proxy.provider.hdfs-ha": "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"
			},
                        "fileType": "text",
                        "path": "/user/hive/warehouse/ods.db/datax_dept",
                        "fileName": "202104",
                        "column": [
                            {
                                "name": "deptno",
                                "type": "int"
                            },
                            {
                                "name": "dname",
                                "type": "varchar"
                            },
                            {
                                "name": "loc",
                                "type": "varchar"
                            }
                        ],
                        "writeMode": "append",
                        "fieldDelimiter": "\t"
                    }
                }
            }
        ]
    }
}
```

#### 4.2.3 执行

```bash
python datax.py mysql2hive.json
```

登录hive客户端查看hive表数据

```bash
$ beeline -u jdbc:hive2://hadoop-node1:11000  -n root
$ select * from datax.hive_person;
```

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/794f994c7a634acfa7d126572ff3a929~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

### 4.3 HDFS to MYSQL

#### 4.3.1 准备好HDFS文件数据

```bash
$ cd $DATAX_HOME
$ cat >./test/person2.txt<<EOF
1,p1,21
2,p2,22
3,p3,30
4,p4,35
5,p5,31
6,p6,33
EOF

# 将文件推送到HDFS上
$ hadoop fs -put ./test/person2.txt /tmp/datax/
```

#### 4.3.2 准备好MySQL表

```bash
CREATE TABLE IF NOT EXISTS `datax`.`person2` (
 `id` int(10) NOT NULL AUTO_INCREMENT COMMENT 'ID',
 `name` VARCHAR(32) COMMENT '用户名',
 `age` int(10) COMMENT '年龄',
 PRIMARY KEY (`id`)
)ENGINE=INNODB DEFAULT CHARSET=utf8;
```

#### 4.3.3 配置json文件

```bash
$ cat >./test/hdfs2mysql.json<<EOF
{
    "job": {
        "setting": {
            "speed": {
                 "channel":1
            }
        },
        "content": [
            {
                "reader": {
                    "name": "hdfsreader",
                    "parameter": {
                        "path": "/tmp/datax/person2.txt",
                        "defaultFS": "hdfs://hadoop-node1:8082",
                        "fileType": "text",
                        "column": [
                               {
                                "index": 0,
                                "type": "long"
                               },
                               {
                                "index": 1,
                                "type": "string"
                               },
                               {
                                "index": 2,
                                "type": "long"
                               }
                        ],
                        "encoding": "UTF-8",
                        "fieldDelimiter": ","
                    }
                },
                "writer": {
                    "name": "mysqlwriter",
                    "parameter": {
                        "writeMode": "insert",
                        "username": "root",
                        "password": "123456",
                        "column": [
                            "id",
                            "name",
                            "age"
                        ],
                        "preSql": [
                            "delete from person2"
                        ],
                        "connection": [
                            {
                                "jdbcUrl": "jdbc:mysql://hadoop-node1:3306/datax?characterEncoding=utf8&useSSL=false&serverTimezone=UTC&rewriteBatchedStatements=true",
                                "table": [
                                    "person2"
                                ]
                            }
                        ]
                    }
                }
            }
        ]
    }
}
EOF
```

#### 4.3.4 执行

```bash
$ python2 ./bin/datax.py ./test/hdfs2mysql.json
```

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4b2162ddee894d00ba15fe157e85ffec~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

#### 4.3.5 验证

登录mysql查看

```bash
$ mysql -uroot -p
密码：123456
select * from datax.person2;
```

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a0e79ce7584e4bd88e065520c98a0396~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

### 4.4 DataX到HBase

- [使用Datax同步mysql,oracle,sqlserver数据到Hbase](https://blog.csdn.net/eyeofeagle/article/details/107938254)
- [dataX案例-从mysql读取数据，写入到hbase中](https://blog.csdn.net/qq_41712271/article/details/108610591)

## 六、DataX-WEB 安装部署

GitHub地址：[github.com/WeiYe-Jing/…](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FWeiYe-Jing%2Fdatax-web)

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c296925c391e49a09085aaeb1c96e7ae~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

### 1）下载

下载地址：

> [pan.baidu.com/share/init?…](https://link.juejin.cn?target=https%3A%2F%2Fpan.baidu.com%2Fshare%2Finit%3Fsurl%3D3yoqhGpD00I82K4lOYtQhg) 提取码：cpsk

### 2）解压

```bash
$ cd /opt/bigdata/hadoop/software
$ tar -xf datax-web-2.1.2.tar.gz -C /opt/bigdata/hadoop/server/
```

### 3）配置环境变量

```bash
$ cd /opt/bigdata/hadoop/server/datax-web-2.1.2
$ vi /etc/profile
export DATAXWEB_HOME=/opt/bigdata/hadoop/server/datax-web-2.1.2
export PATH=$DATAXWEB_HOME/bin:$PATH
$ source /etc/profile
```

### 4）创建dataxweb数据库

```bash
$ mysql -uroot -p -hhadoop-node1
密码：123456
create database dataxweb;
```

### 5）执行一键安装脚本

```bash
$ cd $DATAXWEB_HOME
$ ./bin/install.sh
```

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6cba6923fa00414aac2cd93976a444da~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

### 6）修改配置

#### 1、修改datax-admin配置

```bash
$ cd $DATAXWEB_HOME
# 修改数据库配置，如果上面配置了，就可以跳过
$ vi ./modules/datax-admin/conf/bootstrap.properties
# 配置环境变量
$ vi ./modules/datax-admin/bin/env.properties
# web端口
SERVER_PORT=18088

# 创建 mybatis-plus打印sql日志默认目录，默认路径：$ $DATAXWEB_HOME/modules/datax-admin/data/applogs/admin，要修改就这个配置文件：$DATAXWEB_HOME/modules/datax-admin/conf/application.yml
$ mkdir -p $DATAXWEB_HOME/modules/datax-admin/data/applogs/admin
```

#### 2、修改datax-executor配置

```bash
$ cd $DATAXWEB_HOME
# 修改数据库配置，如果上面配置了，就可以跳过
$ vi ./modules/datax-executor/conf/bootstrap.properties
# 配置环境变量
$ vi ./modules/datax-executor/bin/env.properties
# 主要修改配置如下：
## PYTHON脚本执行位置
PYTHON_PATH=/opt/bigdata/hadoop/server/datax/bin/datax.py
## 保持和datax-admin端口一致，更datax-admin的SERVER_PORT对应
DATAX_ADMIN_PORT=18088

# 创建 日志默认目录，默认路径：$DATAXWEB_HOME/modules/datax-executor/data/applogs/executor/jobhandler，要修改就这个配置文件：$DATAXWEB_HOME/modules/datax-executor/conf/application.yml
$ mkdir -p $DATAXWEB_HOME/modules/datax-executor/data/applogs/executor/jobhandler
```

### 7）启动服务

```bash
$ cd $DATAXWEB_HOME
$ ./bin/start-all.sh
# 或者分模块启动
$ ./bin/start.sh -m datax-admin
$ ./bin/start.sh -m datax-executor

# 查看datax-admin启动日志
$DATAXWEB_HOME/modules/datax-admin/bin/console.out
# 查看datax-executor启动日志
$DATAXWEB_HOME/modules/datax-executor/bin/console.out
```

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aa3833d032544a1594b11381b94fddb6~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

web访问：[http://hadoop-node1:18088/index.html](https://link.juejin.cn?target=http%3A%2F%2Fhadoop-node1%3A18088%2Findex.html) 默认账号/密码：admin/123456 

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/19d373eb574942ebbd2c026b9607e93f~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

### 8）简单使用

#### 前期准备

1、新建项目 

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/423b4703d3a047ab82dcd4515c0f532c~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

2、创建hive库和表

```bash
$ beeline
create database dataxweb;
CREATE TABLE  IF NOT EXISTS dataxweb.hive_person(
id INT COMMENT 'ID',
name STRING COMMENT '名字',
age INT COMMENT '年龄'
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';
```

3、创建dataxweb person表

```sql
CREATE TABLE `dataxweb`.`person` (
  `id` int NOT NULL AUTO_INCREMENT COMMENT 'ID',
  `name` varchar(32) DEFAULT NULL COMMENT '用户名',
  `age` int DEFAULT NULL COMMENT '年龄',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8mb3;
```

#### 1、MYSQL to Hive

创建任务 

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0c4f92466832454394d9180a7bdbc3d2~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

json配置如下：

```json
{
    "job": {
        "setting": {
            "speed": {
                 "channel":1
            }
        },
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "username": "root",
                        "password": "123456",
                        "connection": [
                            {
                                "querySql": [
                                    "select * from datax.person;"
                                ],
                                "jdbcUrl": [
                                    "jdbc:mysql://hadoop-node1:3306/dataxweb?characterEncoding=utf8&useSSL=false&serverTimezone=UTC&rewriteBatchedStatements=true"
                                ]
                            }
                        ]
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "defaultFS": "hdfs://hadoop-node1:8082",
                        "fileType": "text",
                        "path": "/user/hive/warehouse/dataxweb.db/hive_person",
                        "fileName": "person",
                        "column": [
                            {
                                "name": "id",
                                "type": "INT"
                            },
                            {
                                "name": "name",
                                "type": "STRING"
                            },
                            {
                                "name": "age",
                                "type": "INT"
                            }
                        ],
                        "writeMode": "append",
                        "fieldDelimiter": ","
                    }
                }
            }
        ]
    }
}
```

执行，也可以定时执行 

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/95688dc4c087460681af4fc9df36c5e8~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

查看日志 

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1091f21613c547249ed8b120edd7814f~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

#### 2、Hive to MYSQL

创建任务 

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9cc9a34e4c0a4771ab57d7c802933ec1~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

json配置如下：

```json
{
  "job": {
    "setting": {
      "speed": {
        "channel": 1
      }
    },
    "content": [
      {
        "reader": {
          "name": "hdfsreader",
          "parameter": {
            "path": "/user/hive/warehouse/dataxweb.db/hive_person/person__7c10087d_a834_4558_b830_26322bad724b",
            "defaultFS": "hdfs://hadoop-node1:8082",
            "fileType": "text",
            "column": [
              {
                "index": 0,
                "type": "long"
              },
              {
                "index": 1,
                "type": "string"
              },
              {
                "index": 2,
                "type": "long"
              }
            ],
            "encoding": "UTF-8",
            "fieldDelimiter": ","
          }
        },
        "writer": {
          "name": "mysqlwriter",
          "parameter": {
            "writeMode": "insert",
            "username": "root",
            "password": "123456",
            "column": [
              "id",
              "name",
              "age"
            ],
            "preSql": [
              "delete from dataxweb.person"
            ],
            "connection": [
              {
                "jdbcUrl": "jdbc:mysql://hadoop-node1:3306/dataxweb?characterEncoding=utf8&useSSL=false&serverTimezone=UTC&rewriteBatchedStatements=true",
                "table": [
                  "person"
                ]
              }
            ]
          }
        }
      }
    ]
  }
}
```

执行，也可以定时执行 

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f88d5ccc9a234f01938e2dad58b5f4a4~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

查看日志 

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e3e8dc2427c2483eb1ff44df8a9197bc~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.image)

其实知道上面datax命令操作，web端操作就非常简单了，这里只是简单的实现了两个示例，其它的小伙伴也可以试试，也非常简单

> 【温馨提示】执行机必须要有python环境变量哦！！！

## 七、DataX和Sqoop的比较

- [【知识】ETL大数据集成工具Sqoop、dataX、Kettle、Canal、StreamSets大比拼](https://cloud.tencent.com/developer/article/2002618)

关于Sqoop，可以参考我之前的文章：[大数据Hadoop之——数据转换工具Sqoop](https://juejin.cn/post/7100541045003255822)

### 1）Sqoop主要特点

- 可以将关系型数据库中的数据导入hdfs、hive或者hbase等hadoop组件中，也可将hadoop组件中的数据导入到关系型数据库中；
- Sqoop在导入导出数据时，充分采用了map-reduce计算框架，根据输入条件生成一个map-reduce作业，在hadoop集群中运行；
- **采用map-reduce框架**同时在多个节点进行import或者export操作，速度比单节点运行多个并行导入导出效率高，同时提供了良好的并发性和容错性；
- 支持insert、update模式，可以选择参数，若内容存在就更新，若不存在就插入；
- 对国外的主流关系型数据库支持性更好。

### 2）DataX主要特点

- 异构数据库和文件系统之间的数据交换；
- **采用Framework + plugin架构构建**，Framework处理了缓冲，流控，并发，上下文加载等高速数据交换的大部分技术问题，提供了简单的接口与插件交互，插件仅需实现对数据处理系统的访问；
- 数据传输过程在单进程内完成，**全内存操作**，不读写磁盘，也没有IPC；
- 开放式的框架，开发者可以在极短的时间开发一个新插件以快速支持新的数据库/文件系统。

### 3）Sqoop和DataX的区别

- **`Sqoop`采用map-reduce计算框架**进行导入导出，而datax仅仅在运行datax的单台机器上进行数据的抽取和加载，速度比`Sqoop`慢了许多；
- **`Sqoop`只可以在关系型数据库和hadoop组件之间进行数据迁移**，而在hadoop相关组件之间，比如hive和hbase之间就无法使用`Sqoop`互相导入导出数据，同时在关系型数据库之间，比如mysql和oracle之间也无法通过sqoop导入导出数据；
- 与之相反，**`DataX`能够分别实现关系型数据库和hadoop组件之间、关系型数据库之间、hadoop组件之间的数据迁移**；
- `Sqoop`是专门为hadoop而生，对hadoop支持度好，而`DataX`可能会出现不支持高版本hadoop的现象；
- `Sqoop`只支持官方提供的指定几种关系型数据库和hadoop组件之间的数据交换，而在`DataX`中，用户只需根据自身需求修改文件，生成相应rpm包，自行安装之后就可以使用自己定制的插件；
- `Sqoop`不支持ORC文件格式，而`DataX`支持。

Sqoop和DataX各有优缺点，根据应用场景选择，如有什么疑问欢迎给我留言，后续会有更多关于大数据的文章。

## 八、DataX优化

- [datax 优化设置](https://www.cnblogs.com/muzhongjiang/p/13892826.html)
- [DataX系列10-DataX优化](https://www.jianshu.com/p/2713e2679232)
- [DataX优化](https://blog.csdn.net/qq_36593748/article/details/122441642)

### 8.1 Datax的执行过程

要想进行调优，一般先要了解执行过程，执行过程如下：
![img](https://images.xiaozhuanlan.com/photo/2018/86cb476b0bce571db3852d0572612c6d.png)

------

过程详细说明如下：

1. DataX完成单个数据同步的作业，我们称之为Job，DataX接受到一个Job之后，将启动一个进程来完成整个作业同步过程。DataX Job模块是单个作业的中枢管理节点，承担了数据清理、子任务切分(将单一作业计算转化为多个子Task)、TaskGroup管理等功能。
2. DataXJob启动后，会根据不同的源端切分策略，将Job切分成多个小的Task(子任务)，以便于并发执行。Task便是DataX作业的最小单元，每一个Task都会负责一部分数据的同步工作。
3. 切分多个Task之后，DataX Job会调用Scheduler模块，根据配置的并发数据量，将拆分成的Task重新组合，组装成TaskGroup(任务组)。每一个TaskGroup负责以一定的并发运行完毕分配好的所有Task，默认单个任务组的并发数量为5。
4. 每一个Task都由TaskGroup负责启动，Task启动后，会固定启动Reader—>Channel—>Writer的线程来完成任务同步工作。
5. DataX作业运行起来之后， Job监控并等待多个TaskGroup模块任务完成，等待所有TaskGroup任务完成后Job成功退出。否则，异常退出，进程退出值非0

简单总结过程如下：

一个DataX Job会切分成多个Task，每个Task会按TaskGroup进行分组，一个Task内部会有一组Reader->Channel->Writer。Channel是连接Reader和Writer的数据交换通道，所有的数据都会经由Channel进行传输

### 8.2 DataX优化

优化1：提升每个channel的速度

在DataX内部对每个Channel会有严格的速度控制，分两种，一种是控制每秒同步的记录数，另外一种是每秒同步的字节数，默认的速度限制是1MB/s，可以根据具体硬件情况设置这个byte速度或者record速度，一般设置byte速度，比如：我们可以把单个Channel的速度上限配置为5MB

**优化2：**提升DataX Job内Channel并发数 并发数=taskGroup的数量*每一个TaskGroup并发执行的Task数 (默认单个任务组的并发数量为5)。

提升job内Channel并发有三种配置方式：

1. 配置全局Byte限速以及单Channel Byte限速，Channel个数 = 全局Byte限速 / 单Channel Byte限速
2. 配置全局Record限速以及单Channel Record限速，Channel个数 = 全局Record限速 / 单Channel Record限速
3. 直接配置Channel个数.

配置含义：

```java
job.setting.speed.channel : channel并发数
job.setting.speed.record : 全局配置channel的record限速
job.setting.speed.byte：全局配置channel的byte限速

core.transport.channel.speed.record：单channel的record限速
core.transport.channel.speed.byte：单channel的byte限速
```

#### 8.2.1 方式1

举例如下：core.transport.channel.speed.byte=1048576，job.setting.speed.byte=5242880，所以Channel个数 = 全局Byte限速 / 单Channel Byte限速=5242880/1048576=5个，配置如下：

```json
{
    "core": {
        "transport": {
            "channel": {
                "speed": {
                    "byte": 1048576
                }
            }
        }
    },
    "job": {
        "setting": {
            "speed": {
                "byte" : 5242880
            }
        },
        ...
    }
}
```

#### 8.2.2 方式2

举例如下：core.transport.channel.speed.record=100，job.setting.speed.record=500,所以配置全局Record限速以及单Channel Record限速，Channel个数 = 全局Record限速 / 单Channel Record限速=500/100=5

```json
{
    "core": {
        "transport": {
            "channel": {
                "speed": {
                    "record": 100
                }
            }
        }
    },
    "job": {
        "setting": {
            "speed": {
                "record" : 500
            }
        },
        ...
    }
}
```

#### 8.2.3 方式3

举例如下：直接配置job.setting.speed.channel=5，所以job内Channel并发=5个

```json
{
    "job": {
        "setting": {
            "speed": {
                "channel" : 5
            }
        },
        ...
    }
}
```

### 8.3 注意事项

- 当提升DataX Job内Channel并发数时，调整JVM heap参数，原因如下：

  - 当一个Job内Channel数变多后，内存的占用会显著增加，因为DataX作为数据交换通道，在内存中会缓存较多的数据。

  - 例如Channel中会有一个Buffer，作为临时的数据交换的缓冲区，而在部分Reader和Writer的中，也会存在一些Buffer，为了防止jvm报内存溢出等错误，调大jvm的堆参数。

  - 通常我们建议将内存设置为4G或者8G，这个也可以根据实际情况来调整

  - 调整JVM xms xmx参数的两种方式：一种是直接更改datax.py；另一种是在启动的时候，加上对应的参数，如下： 
  `python datax/bin/datax.py --jvm="-Xms8G -Xmx8G" XXX.json`

- Channel个数并不是越多越好， 原因如下：

  - Channel个数的增加，带来的是更多的CPU消耗以及内存消耗。

  - 如果Channel并发配置过高导致JVM内存不够用，会出现的情况是发生频繁的Full GC，导出速度会骤降，适得其反。这个可以通过观察日志发现

> 注意：
>
> MysqlReader进行数据抽取时，如果指定splitPk，表示用户希望使用splitPk代表的字段进行数据分片，DataX因此会启动并发任务进行数据同步，这样可以大大提供数据同步的效能，splitPk不填写，包括不提供splitPk或者splitPk值为空，DataX视作使用单通道同步该表数据，第三个测试不配置splitPk测试不出来效果

### 8.4 优化概述

当觉得DataX传输速度慢时，需要从上述四个方面着手开始排查。

1. 网络本身的带宽等硬件因素造成的影响；
2. DataX本身的参数；
3. 从源端到任务机；
4. 从任务机到目的端；

#### 8.4.1 网络带宽等硬件因素困扰

此部分主要需要了解网络本身的情况，即从源端到目的端的带宽是多少（实际带宽计算公式），平时使用量和繁忙程度的情况，从而分析是否是本部分造成的速度缓慢。

以下提供几个思路。

1. 可使用从源端到目的端scp，python http,nethogs等观察实际网络及网卡速度；
2. 结合监控观察任务运行时间段时，网络整体的繁忙情况，来判断是否应将任务避开网络高峰运行；
3. 观察任务机的负载情况，尤其是网络和磁盘IO，观察其是否成为瓶颈，影响了速度；

### 8.5 DataX本身的参数调优

#### 8.5.1 全局

datax 安装目录的conf 目录下的 core.json 文件。

```ruby
{
   "core":{
        "transport":{
            "channel":{
                "speed":{
                    "channel": 2, ## 此处为数据导入的并发度，建议根据服务器硬件进行调优
                    "record":-1, ##此处解除对读取行数的限制
                    "byte":-1, ##此处解除对字节的限制
                    "batchSize":2048 ##每次读取batch的大小
                }
            }
        }
    },
    "job":{
            ...
        }
    }
```

#### 8.5.2 局部

实际运行每个人物的json配置文件

```ruby
"setting": {
            "speed": {
                "channel": 2,
                "record":-1,
                "byte":-1,
                "batchSize":2048
            }
        }
    }
}
```

channel增大，为防止OOM，需要修改datax工具的datax.py文件。

如下所示，可根据任务机的实际配置，提升-Xms与-Xmx，来防止OOM。

tunnel并不是越大越好，过分大反而会影响宿主机的性能。

```bash
DEFAULT_JVM = "-Xms1g -Xmx1g -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=%s/log" % (DATAX_HOME)
```

#### 8.5.3 Jvm 调优

```bash
python datax.py  --jvm="-Xms3G -Xmx3G" ../job/test.json
```

- -Xms3G 表示JVM的初始值为3G

- -Xmx3G 表示JVM可使用的最大值为3G

这样做的好处是给定一个大的内存，让同步数据处理起来更快， 也可以避免内存的抖动。

### 8.6 DataX优化案例

- [datax优化之reader提速-详细经历](https://blog.csdn.net/cclovezbf/article/details/110928702)

#### 8.6.1 mysql表切分

如果源端是mysql的话，可以使用mysql的切分，并行处理。

```json
{
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "username": "root",
                        "password": "abc123",
                        "column": [
                            "id",
                            "sale_date",
                            "prod_name",
                            "sale_nums"
                        ],
                        "splitPk": "id",
                        "connection": [
                            {
                                "table": [
                                    "fact_sale"
                                ],
                                "jdbcUrl": [
     "jdbc:mysql://10.31.1.122:3306/test"
                                ]
                            }
                        ]
                    }
                },
```

可以看到日志里面根据spilit进行切分了

#### 8.6.2 速度控制

DataX3.0提供了包括通道(并发)、记录流、字节流三种流控模式，可以随意控制你的作业速度，让你的作业在数据库可以承受的范围内达到最佳的同步速度。

关键优化参数如下：

| 参数                                | 说明                                                |
| ----------------------------------- | --------------------------------------------------- |
| job.setting.speed.channel           | 总并发数                                            |
| job.setting.speed.record            | 总record限速                                        |
| job.setting.speed.byte              | 总byte限速                                          |
| core.transport.channel.speed.record | 单个channel的record限速，默认值为10000（10000条/s） |
| core.transport.channel.speed.byte   | 单个channel的byte限速，默认值1024*1024（1M/s）      |

**注意事项：**

1. 若配置了总record限速，则必须配置单个channel的record限速
2. 若配置了总byte限速，则必须配置单个channe的byte限速
3. 若配置了总record限速和总byte限速，channel并发数参数就会失效。因为配置了总record限速和总byte限速之后，实际channel并发数是通过计算得到的：

**计算公式为:**

min(总byte限速/单个channle的byte限速，总record限速/单个channel的record限速)

**配置示例：**

```json
{
    "core": {
        "transport": {
            "channel": {
                "speed": {
                    "byte": 1048576 //单个channel byte限速1M/s
                }
            }
        }
    },
    "job": {
        "setting": {
            "speed": {
                "byte" : 5242880 //总byte限速5M/s
            }
        },
        ...
    }
}
```

#### 8.6.3 内存调整

 当提升DataX Job内Channel并发数时，内存的占用会显著增加，因为DataX作为数据交换通道，在内存中会缓存较多的数据。例如Channel中会有一个Buffer，作为临时的数据交换的缓冲区，而在部分Reader和Writer的中，也会存在一些Buffer，为了防止OOM等错误，需调大JVM的堆内存。

 调整JVM xms xmx参数的两种方式：一种是直接更改datax.py脚本；另一种是在启动的时候，加上对应的参数，如下：

```shell
python datax/bin/datax.py --jvm="-Xms8G -Xmx8G" /path/to/your/job.json
```

- -Xms8G：运行的最小分配内存，如果可用内存不足8G，这个命令将不能被执行。
- -Xmx8G：运行时最大占用内存大小。

#### 8.6.4 mysql querysql优化

querysql。这种方式适合oracle的分区表，或者mysql 有时间字段的表。

一般来说每天的数据量是差不多的。比如我这个表有1000w数据2022-01到2022-02 我可以写12个sql 从1月到12月 这样任务基本均匀分布，也不用像splitPk的那样对字段的要求较高。

```json
{
	"job": {
		"setting": {
			"speed": {
				"channel": 10
			},
			"errorLimit": {
				"record": 0,
				"percentage": 0.02
			}
		},
		"content" : [ {
			"reader" : {
				"name" : "postgresqlreader",
				"parameter" : {
					"username" : "${username}",
					"password" : "${password}",
					"connection" : [ {
						"querySql": [
							"SELECT * FROM db.table1 where period_id='202101'",
							"SELECT * FROM db.table1  where period_id='202102'",
							"SELECT * FROM db.table1  where period_id='202103'",
							"SELECT * FROM db.table1  where period_id='202104'",
							"SELECT * FROM db.table1  where period_id='202105'"
						],
						"jdbcUrl" : [ "${jdbcUrl}" ]
					}]
				}
			},
			"writer": {
				"name": "hdfswriter",
				"parameter": {
					"defaultFS" : "hdfs://${defaultFS}",
					"fileType": "orc",
					"path": "/user/hive/warehouse/dwintdata.db/table1/",
					"fileName": "table1",
					"writeMode": "append",
					"fieldDelimiter": "\u0001",
					"haveKerberos":"true",
					"kerberosKeytabFilePath":"${kerberosKeytabFilePath}",
					"kerberosPrincipal":"${kerberosPrincipal}",
					"hadoopConfig" : {
						"dfs.client.failover.proxy.provider.${defaultFS}" : "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider",
						"dfs.ha.namenodes.${defaultFS}" : "nn1,nn2",
						"dfs.namenode.rpc-address.${defaultFS}.nn2" : "${dfsnn2}",
						"dfs.namenode.rpc-address.${defaultFS}.nn1" : "${dfsnn1}",
						"dfs.nameservices" : "${defaultFS}"
					},
					"column": [
						{"name": "name", "type": "string"},
						{"name": "age", "type": "string"},
						{"name": "period_id", "type": "string"}
					]
				}
			}
		}]
	}
}
```

## DataX系列

- [DataX 全系列之一 —— DataX 安装和使用 - 掘金 (juejin.cn)](https://juejin.cn/post/7006617025816559653)
- [DataX全系列之二 —— DataX 总体架构和原理 - 掘金 (juejin.cn)](https://juejin.cn/post/7006658351375335431)
- [DataX 全系列之三 —— DataX 源码运行流程分析 - 掘金 (juejin.cn)](https://juejin.cn/post/7006619232641220616)
- [DataX 全系列之四 —— DataX 核心数据结构 - 掘金 (juejin.cn)](https://juejin.cn/post/7007616850745884708)
- [DataX 全系列之五 —— DataX-web 介绍和使用 - 掘金 (juejin.cn)](https://juejin.cn/post/7006658574529069086)

- [datax开发hdfswriter写入目录不存在时自动创建](https://blog.csdn.net/Mr_ShangHaohao/article/details/121974596)

## DataX与Kettle对比

- [浅测评DataX与Kettle_敲代码的蒜子的博客-CSDN博客_datax和kettle哪个好](https://blog.csdn.net/weixin_44411398/article/details/116695232)

- [Datax和Kettle使用场景的对比_击水三千里的博客-CSDN博客_datax和kettle哪个好](https://blog.csdn.net/lzhcoder/article/details/120830522)

| 较维度\产品    | Kettle                                                       | DataX                                                        |                             |
| :------------- | :----------------------------------------------------------- | :----------------------------------------------------------- | --------------------------- |
| 设计及架构     | 适用场景                                                     | 面向数据仓库建模传统ETL工具                                  | 面向数据仓库建模传统ETL工具 |
| 支持数据源     | 多数关系型数据库                                             | 少数关系型数据库和大数据非关系型数据库                       |                             |
| 开发语言       | Java                                                         | Python、Java                                                 |                             |
| 可视化web界面  | KettleOnline代码收费Kettle-manager代码免费                   | Data-Web代码免费                                             |                             |
| 底层架构       | 主从结构非高可用，扩展性差，架构容错性低，不适用大数据场景   | 支持单机部署和第三方调度的集群部署两种方式                   |                             |
| 功能           | CDC机制                                                      | 基于时间戳、触发器等                                         | 离线批处理                  |
| 抽取策略       | 支持增量，全量抽取                                           | 支持全量抽取。不支持增量抽取要通过shell脚本自己实现          |                             |
| 对数据库的影响 | 对数据库表结构有要求，存在一定侵入性                         | 通过sql select 采集数据，对数据源没有侵入性                  |                             |
| 自动断点续传   | 不支持                                                       | 不支持                                                       |                             |
| 数据清洗       | 围绕数据仓库的数据需求进行建模计算，清洗功能相对复杂，需要手动编程 | 需要根据自身清晰规则编写清洗脚本，进行调用（DataX3.0 提供的功能）。 |                             |
| 数据转换       | 手动配置schema mapping                                       | 通过编写json脚本进行schema mapping映射                       |                             |
| 特性           | 数据实时性                                                   | 非实时                                                       | 定时                        |
| 应用难度       | 高                                                           | 高                                                           |                             |
| 是否需要开发   | 是                                                           | 是                                                           |                             |
| 易用性         | 低                                                           | 低                                                           |                             |
| 稳定性         | 低                                                           | 中                                                           |                             |
| 抽取速度       | 小数据量的情况下差别不大，大数据量时datax比kettle快。datax对于数据库压力比较小 |                                                              |                             |
| 其他           | 实施及售后服务                                               | 开源软件，社区活跃度高                                       | 阿里开源代码，社区活跃度低  |