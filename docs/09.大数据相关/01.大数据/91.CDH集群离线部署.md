---
title: CDH集群离线部署
date: 2022-10-26 09:38:36
permalink: /bigdata/cdh01/
categories: 
  - 大数据
tags: 
  - 大数据
---

- [CDH集群离线部署(CM6.3.1 + CDH6.3.2 + CentOS7)_](https://blog.csdn.net/qq_40856560/article/details/109007683)
- [CDH 6.3.2 离线安装部署详细教程](https://blog.csdn.net/BoomLee/article/details/119119096)
- [【CDH实战01】基于CentOS7的CDH6.3.2完全分布式集群搭建【上】](http://www.amoscloud.com/?p=2638)
- [【CDH实战02】基于CentOS7的CDH6.3.2完全分布式集群搭建【下】 ](http://www.amoscloud.com/?p=2839)
- [CDH6.3.2离线安装（附百度网盘CDH安装包）](https://zhuanlan.zhihu.com/p/366308900)
- [CM6.3.1+CDH6.3.2环境搭建](https://zhuanlan.zhihu.com/p/425449993)
- [CDH6.3.2 详细介绍及使用](https://zhuanlan.zhihu.com/p/428044940)
- [大数据之CDH（web页面部署Hadoop）](https://blog.csdn.net/yanxifaner/article/details/120210259)
- [CDH集群部署最佳实践](https://zhuanlan.zhihu.com/p/72167108)
- [CDH 6.3 大数据平台搭建 ](https://zhuanlan.zhihu.com/p/444565129)
- [CDH6.3.1集群离线部署](https://juejin.cn/post/6844904052698906631)
- [基于阿里云的CDH集群安装_Frank](https://blog.csdn.net/qq_16906867/article/details/127519913)
- [CDH简介及CDH部署、原理和使用介绍( 版本6.3.1 )_](https://blog.csdn.net/wt334502157/article/details/120290580)
- [【手册】CDH6.3.2及hadoop生态圈工具安装部署手册（附带安装包）](https://blog.csdn.net/spark9527/article/details/116757508)
- [大数据之CDH（web页面部署Hadoop）](https://blog.csdn.net/yanxifaner/article/details/120210259)
- [CDH大数据平台搭建之集群规划](https://blog.csdn.net/qq_41924766/article/details/117561341)
- [CDH安装Phoenix(cdh6.2以上版本、镜像包和安装文档)_](https://blog.csdn.net/qq_43016289/article/details/118998355)
- [ClouderaManager6.3.1+CDH6.3.2+PHOENIX-5.0.0集成部署](https://juejin.cn/post/6901675561685516295)

## 1 原生Hadoop的问题

1. 版本管理过于混乱
2. 部署过程较为繁琐,升级难度较大
3. 兼容性差
4. 安全性低

## 2 CDH和CM(Cloudera Manager）

1. CDH(Cloudera’s Distribution Including Apache Hadoop),是Hadoop众多分中的一种，由Cloudera公司维护，基于稳定版本的Apache Hadoop构建，并集成了很多补丁，可以直接用于生产环境。就是Hadoop等大数据安装包的第三方版本的集合，提供了Hadoop等大数据服务的安装包。
2. CM(Cloudera Manager)提供了一个管理和监控Hadoop等大数据服务的web界面，能让我们方便安装大数据生态圈的大部分服务。

## 3 Hadoop自动化部署和管理平台

主流的有Apache Ambari和Cloudera Manager，相对应的Hadoop的发行版为HDP和CDH。

这种自动化部署平台的功能一般如下:

1. 提供Hadoop大数据集群
2. 管理Hadoop大数据集群
3. 监控Hadoop大数据集群

PS:HDP的公司(hortonworks)已经被CDH公司(Cloudera)收购了

## 4 Cloudera Manager架构

![img](https://img-blog.csdnimg.cn/20201011090404505.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODU2NTYw,size_16,color_FFFFFF,t_70)



1. Server：负责软件安装、配置，启动和停止服务，管理服务运行的群集。**核心**

2. Agent：安装在每台主机上。负责**启动和停止进程，**配置，监控主机。

3. Management Service：由一组执行各种监控，警报和报告功能角色的服务。**图表的生成和管理**

4. Database：存储配置和监视信息。

5. Cloudera Repository：软件由Cloudera 管理分布存储库。（有点**类似Maven的中心仓库**）；在线安装（从中心仓库拉取）和离线安装（离线库）

6. Clients：是用于与服务器进行交互的接口（API和Admin Console）

## 5 CDH下载

官方下载地址:https://archive.cloudera.com

### 5.1 CM下载

https://archive.cloudera.com/cm6/6.3.1/redhat7/yum/RPMS/x86_64/

![img](https://img-blog.csdnimg.cn/20201011090420512.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODU2NTYw,size_16,color_FFFFFF,t_70)



### 5.2 CDH下载

https://archive.cloudera.com/cdh6/6.3.2/parcels/

![img](https://img-blog.csdnimg.cn/20201011090432419.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODU2NTYw,size_16,color_FFFFFF,t_70)

注意:CDH的版本一定要和CM的版本对应

## 6 环境准备

使用VMware模拟多台主机，由于主机条件有限，只演示三台机器，配置如下:

| 主机名 | 系统    | IP             | 内存 | 磁盘 |
| ------ | ------- | -------------- | ---- | ---- |
| cdh-1  | Centos7 | 192.168.100.10 | 4G   | 60G  |
| cdh-2  | Centos7 | 192.168.100.20 | 2G   | 60G  |
| cdh-3  | Centos7 | 192.168.100.30 | 2G   | 60G  |

### 6.1 修改主机名(所有节点)

```bash
hostnamectl set-hostname cdh-1
hostnamectl set-hostname cdh-2
hostnamectl set-hostname cdh-3
```

### 6.2 关闭防火墙(所有节点)

```bash
systemctl stop firewalld
systemctl disable firewalld
```

### 6.3 关闭SELinux(所有节点)

```bash
setenforce 0 #临时关闭
#永久关闭 将SELINUX= enforcing 修改为SELINUX=disabled
vi /etc/selinux/config
SELINUX=disabled 
```

PS: 可以使用

```bash
sed -i s/SELINUX=enforcing/SELINUX=disabled/g /etc/selinux/config
```

### 6.4 配置IP到主机的映射(所有节点)

```bash
vi /etc/hosts
192.168.100.10 cdh-1
192.168.100.20 cdh-2
192.168.100.30 cdh-3
```

### 6.5 配置免密码登录(cdh-1)

```bash
# 生成公钥和私钥 三次回车
ssh-keygen
# 复制公钥和私钥
ssh-copy-id cdh-1
ssh-copy-id cdh-2
ssh-copy-id cdh-3
```

### 6.6 设置用户最大可打开文件数，进程数，内存占用(所有节点)

```bash
vi /etc/security/limits.conf
```

```bash
soft    nofile   32728
hard    nofile   1024999
soft    nproc   65535
hard    noroc    unlimited
soft    memlock    unlimited
hard    memlock    unlimited
```

```bash
sysctl -p
```

### 6.7 设置swap空间(所有节点)

```bash
echo "vm.swappiness = 0" >> /etc/sysctl.conf
```

Cloudera建议将交换空间设置为0，过多的交换空间会引起GC耗时的激增。

### 6.8 关闭大页面压缩(所有节点)

```bash
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag
```

## 7 安装

将下载好的CDH包和CM的包使用sftp上传到cdh-1

### 7.1 配置本地yum

CDH的安装包都是rpm包如果使用rpm安装方式安装起来是比较复杂的，会有很多依赖问题需要解决，就需要使用yum帮助我们解决依赖问题。

![img](https://img-blog.csdnimg.cn/20201011090650551.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODU2NTYw,size_16,color_FFFFFF,t_70)

#### 7.1.1 配置centos源(cdh-1)

```bash
#挂载centos7镜像
mount /dev/cdrom /mnt/
#删除系统自带的源
rm -rf /etc/yum.repos.d/*
#新建一个本地yum源
cat >> /etc/yum.repos.d/local.repo << EOF
[centos]
name=centos
baseurl=file:///mnt
gpgcheck=0
EOF
#验证
yum repolist
```

#### 7.1.2 安装httpd服务（cdh-1）

```bash
# 安装
yum install -y httpd
# 启动
systemctl start httpd
#开机自启
systemctl enable httpd
```

http服务可以帮助我们传输文件，默认静态资源的目录为/var/www/html

#### 7.1.2 centos源配置为http方式获取（cdh-1）

```bash
#在http服务的静态资源目录创建centos目录
mkdir /var/www/html/centos
#将centos的镜像文件复制到centos目录
cp -rvf /mnt/* /var/www/html/centos/
#可以通过http访问了
http://192.168.100.10/centos/
#修改cdh-1的本地centos源的配置
vi /etc/yum.repos.d/local.repo
baseurl=http://cdh-1/centos
#取消挂载
umount /dev/cdrom /mnt
```

其他节点配置(cdh-1,cdh-2)

```bash
cat >> /etc/yum.repos.d/local.repo << EOF
[centos]
name=centos
baseurl=http://cdh-1/centos
gpgcheck=0
EOF
#验证
yum repolist
```

#### 7.1.3 配置CM源

1. 移动文件安装包文件到http服务器静态文件目录(cdh-1)

```bash
#在/var/www/html创建存放cm包的文件夹和cdh安装包的文件夹
mkdir /var/www/html/{cm,cdh}
#将cdh的安装包和cm的包移动到创建的目录
#移动cm安装包
mv cloudera-manager-* /var/www/html/cm/
mv  enterprise-debuginfo-6.3.1-1466458.el7.x86_64.rpm oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm /var/www/html/cm
#移动cdh安装包和元数据文件
mv CDH-6.3.2-1.cdh6.3.2.p0.1605554-el7.parcel manifest.json /var/www/html/cdh/
```

2. 制作CM源生成repodata文件，需要用到createrepo这个包(cdh-1)

```bash
#安装
yum install -y createrepo
#进入到cm的rpm包存放目录
cd /var/www/html/cm
# 生成repodata文件夹
createrepo .
```

3. 配置yum源（所有节点）

```bash
cat >> /etc/yum.repos.d/cm.repo << EOF
[CM]
name=cm
baseurl=http://cdh-1/cm/
gpgcheck=0
EOF
```

### 7.2 安装

![img](https://img-blog.csdnimg.cn/20201011090730501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODU2NTYw,size_16,color_FFFFFF,t_70)

#### 7.2.1 安装依赖(所有节点)

```bash
yum install -y bind-utils libxslt cyrus-sasl-plain cyrus-sasl-gssapi portmap fuse-libs /lib/lsb/init-functions httpd mod_ssl openssl-devel python-psycopg2 Mysql-python fuse
```

#### 7.2.2 安装Cloudera Manager和Cloudera Agent(cdh-1)

```bash
#安装JDK
yum install -y oracle-j2sdk1.8.x86_64
#安装cloudera-manager
yum install  -y cloudera-manager-agent cloudera-manager-daemons cloudera-manager-server cloudera-manager-server-db-2 postgresq-server
```

#### 7.2.3 安装Mariadb

```bash
#安装
yum install -y mariadb-server
#启动和开机自启
systemctl start mariadb && systemctl enable mariadb
#配置Mariadb数据库
mysql_secure_installation
```

![img](https://img-blog.csdnimg.cn/20201011090751784.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODU2NTYw,size_16,color_FFFFFF,t_70)

#### 7.2.4 初始化管理节点(cdh-1)

1. 复制mysql的jdbc驱动包到/usr/share/java目录

需要使用sftp上传jar包到cdh-1节点上

```bash
#创建/usr/share/java目录
mkdir -p /usr/share/java
#复制jar包到/usr/share/java下
cp mysql-connector-java-5.1.48.jar /usr/share/java/
#注意:需要改名为mysql-connector-java.jar
mv /usr/share/java/mysql-connector-java-5.1.48.jar /usr/share/java/mysql-connector-java.jar
```

2. 初始化数据库

```bash
/opt/cloudera/cm/schema/scm_prepare_database.sh mysql -h localhost -uroot -proot --scm-host localhost scm root root
```

#### 7.2.5 安装agent节点

只需要在chd-2和cdh-3节点上安装

```bash
#安装jdk
yum install -y oracle-j2sdk1.8.x86_64
#安装agent
yum install cloudera-manager-daemons cloudera-manager-agent -y
```

#### 7.2.6 修改配置文件(所有节点)

修改Cloudera Agent配置文件/etc/cloudera-scm-agent/config.ini，配置server_host为主节点cdh-1

```bash
#通过vi命令修改
vi /etc/cloudera-scm-agent/config.ini
server_host=cdh-1
#也可以通过sed命令修改
sed -i "s/server_host=localhost/server_host=cdh-1/g" /etc/cloudera-scm-agent/config.ini
注意:只用使用一种命令修改就行了，推荐使用sed
```

#### 7.2.7 配置JAVA_HOME(所有节点)

```bash
vi /etc/profile
```

```bash
export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera/
export PATH=$PATH:$JAVA_HOME/bin
```

### 7.3 启动

#### 7.3.1 启动Cloudera Manager(cdh-1)

在主节点启动Cloudera Manager

```bash
#启动
systemctl start cloudera-scm-server
#设置开机自启
systemctl enable cloudera-scm-server
```

#### 7.3.2 启动Cloudera Agent(所有节点)

```bash
#启动
systemctl start cloudera-scm-agent
#开机自启
systemctl enable cloudera-scm-agent
```

可以访问http://192.168.100.10:7180 用户名密码都是admin

## 8 CDH运维

### 8.1 CDH 常用指令及问题处理

- [CDH 常用指令及问题处理](https://blog.csdn.net/zhang_qings/article/details/123679017)


常用指令：

```bash
## 主节点启动server
systemctl start cloudera-scm-server

## 从节点启动agent
systemctl start cloudera-scm-agent

## 停止
systemctl stop cloudera-scm-server
systemctl stop cloudera-scm-agent
service supervisord stop
systemctl disable cloudera-scm-server

# 关闭开机启动
systemctl disable cloudera-scm-agent 

## 查看日志
tail -F /var/log/cloudera-scm-server/cloudera-scm-server.log

tail -F /var/log/cloudera-scm-agent/cloudera-scm-agent.log

# 服务端口查看
netstat -apn | grep 7180

# 测试数据库是否 正常连接
/opt/cloudera/cm/schema/scm_prepare_database.sh mysql cmf root 123456

# 跑mapreduce 测试：
sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi 1 1
```

#### 8.1.1 操作CM

（1）启动cm

```sql
systemctl start cloudera-scm-server
```

（2）重启cm

```vbscript
systemctl restart cloudera-scm-server
```

（3）停止

```vbscript
systemctl stop cloudera-scm-server
```

#### 8.1.2 查看日志

```bash
tail -f /var/log/kafka/kafka-broker-cdh1.log

tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log
```

### 8.2 端口使用整理

- [CDH6.3.2 端口使用整理](https://blog.csdn.net/Apache_Jerry/article/details/115321227)
- [CDH常用端口汇总](https://www.jianshu.com/p/6f9b9b5f6100)
- [Cloudera Manager和Cloudera Navigator使用的端口](http://www.amoscloud.com/?p=2660)

下图概述了Cloudera Manager，Cloudera Navigator和Cloudera Management Service角色使用的某些端口：

![img](http://www.amoscloud.com/wp-content/uploads/2021/03/image-1616332826172.png)
![file](http://www.amoscloud.com/wp-content/uploads/2021/03/image-1616332921149.png)

| 组件      | 端口  | 说明                                                         |
| --------- | ----- | ------------------------------------------------------------ |
| Hadoop    | 50070 | HDFS WEB UI端口                                              |
|           | 2409  | NFS Gateway 服务器端口                                       |
|           | 4242  | NFS Gateway 服务器角色内执行的装载守护程序的端口号           |
|           | 8020  | 高可用的HDFS RPC端口，NameNode 运行 HDFS 协议的端口。结合 NameNode 的主机名称建立其地址 |
|           | 8022  | HDFS Daemon 可以使用的 service-rpc 地址（而非共享客户端使用的 RPC 地址）的可选端口 |
|           | 9000  | 非高可用的HDFS RPC端口                                       |
|           | 8088  | Yarn 的WEB UI 接口                                           |
|           | 8480  | JournalNode HTTP Web UI 的端口。结合 JournalNode 的主机名称建立其 HTTP 地址 |
|           | 8481  | JournalNode Web UI 侦听的基本端口。结合 JournalNode 的主机名称建立安全的 Web UI 地址 |
|           | 8485  | JournalNode 的RPC端口，结合 JournalNode 的主机名称构建其 RPC 地址 |
|           | 8019  | ZKFC端口                                                     |
|           | 9867  | 各个 DataNode 协议的端口。结合 DataNode 的主机名称建立其 IPC 端口地址 |
|           | 9868  | SecondaryNameNode HTTP 端口。如端口为 0，服务器将在自由端口启动。结合 SecondaryNameNode 的主机名称建立其 HTTP 地址 |
|           | 9869  | 安全 SecondaryNameNode Web UI 侦听的基本端口                 |
|           | 9866  | DataNode 的 XCeiver 协议的端口。结合 DataNode 的主机名称建立其地址 |
|           | 9864  | DataNode HTTP Web UI 的端口。结合 DataNode 的主机名称建立其 HTTP 地址 |
|           | 9865  | DataNode Web UI 侦听的基本端口。结合 DataNode 的主机名称建立安全的 Web UI 地址 |
|           | 9870  | DFS NameNode Web UI 侦听的基本端口。如端口为 0，服务器将在自由端口启动。结合 NameNode 的主机名称建立其 HTTP 地址 |
|           | 9871  | 安全 NameNode Web UI 侦听的基本端口                          |
|           | 10020 | historyserver端口                                            |
|           | 14000 | REST 接口可连接至 HDFS 的端口。如果已为 HttpFS 启用 TLS/SSL，则 REST 接口通过 HTTPS 服务，否则通过 HTTP 服务 |
|           | 14001 | 用于管理界面的端口                                           |
| Zookeeper | 2181  | 客户端连接zookeeper的端口                                    |
|           | 3181  | zookeeper集群内通讯使用，Leader监听此端口                    |
|           | 4181  | zookeeper端口 用于选举leader                                 |
|           | 2888  | zookeeper集群内通讯使用，Leader监听此端口                    |
|           | 3888  | zookeeper端口 用于选举leader                                 |
|           | 9010  | ZooKeeper 服务器 RMI 注册表使用的端口。需要该端口以通过 Cloudera Manager ZooKeeper 监控需要的 RMI 启用 JMX 访问。将其作为“-Dcom.sun.management.jmxremote.port”添加到 ZooKeeper 服务器 JVM 命令行。 |
| Hbase     | 60010 | Hbase的master的WEB UI端口                                    |
|           | 60030 | Hbase的regionServer的WEB UI 管理端口                         |
| Hive      | 9083  | metastore服务默认监听端口                                    |
|           | 10000 | Hive 的JDBC端口                                              |
|           | 10002 | HiveServer2 WebUI 将侦听的端口。可以将其设为 0 以禁用 WebUI  |
|           | 50111 | WebHCat Server 用于监听连接的端口                            |
| Spark     | 7077  | spark 的master与worker进行通讯的端口 standalone集群提交Application的端口 |
|           | 8080  | master的WEB UI端口 资源调度                                  |
|           | 8081  | worker的WEB UI 端口 资源调度                                 |
|           | 4040  | Driver的WEB UI 端口 任务调度                                 |
|           | 18080 | Spark History Server的WEB UI 端口                            |
| kafka     | 9092  | Kafka集群节点之间通信的RPC端口                               |
| CDH       | 5678  | Reports Manager 侦听请求的端口                               |
|           | 7178  | Navigator Metadata Server 监听请求的端口                     |
|           | 7180  | Cloudera Manager WebUI端口                                   |
|           | 7182  | Cloudera Manager Server 与 Agent 通讯端口                    |
|           | 7184  | Event Server 监听事件发布的端口                              |
|           | 7185  | Event Server 监听事件查询的端口                              |
|           | 7186  | Navigator 审核服务器监听请求的端口                           |
|           | 8083  | Reports Manager 启动调试 web 服务器的端口。请设为 -1 以禁用调试服务器 |
|           | 8084  | Event Server 的调试页面的端口。请设为 -1 以禁用调试服务器    |
|           | 8086  | Service Monitor 的调试页面的端口。请设为 -1 以禁用调试服务器 |
|           | 8087  | Activity Monitor Web UI 端口                                 |
|           | 8089  | Navigator 审核服务器启动调试 Web 服务器的端口。请设为 -1 以禁用调试服务器 |
|           | 8091  | Host Monitor 的调试页面的端口。请设为 -1 以禁用调试服务器    |
|           | 9086  | Port for Service Monitor’s HTTPS Debug page                  |
|           | 9091  | Port for Host Monitor’s HTTPS Debug page                     |
|           | 9087  | Port for Activity Monitor’s HTTPS Debug page                 |
|           | 9994  | 公开 Host Monitor 的查询 API 的端口                          |
|           | 9995  | Host Monitor 侦听代理消息的端口                              |
|           | 9996  | 公开 Service Monitor 的查询 API 的端口                       |
|           | 9997  | Service Monitor 侦听代理消息的端口                           |
|           | 9999  | Activity Monitor 侦听端口                                    |
|           | 9998  | 公开 Activity Monitor 的查询 API 的端口                      |
|           | 10101 | Alert Publisher 侦听内部 API 请求的端口                      |
|           | 10110 | The port where Telemetry Publisher listens for requests      |
|           | 10111 | The port where Telemetry Publisher starts a debug web server. Set to -1 to disable debug server |
| HUE       | 8888  | Hue WebUI 端口                                               |

### 8.3 CDH6.3.2集成Phoenix

- [CDH6.3.2集成Phoenix ](https://www.cnblogs.com/gentlescholar/p/16009526.html)
- [CDH6.3.2安装（包括Phoenix和Kylin）](https://blog.csdn.net/monster77777/article/details/109243089)

#### 8.3.1 Phoenix映射HBase中已有的表

本地安装好 Phoenix 之后，用 phoenix 的 `!talblse` 命令列出所有表，会发现 HBase 原有的表没有被列出来。而使用 Phoenix sql 的 `CREATE` 语句创建的一张新表，则可以通过 `!tables` 命令展示出来。

这是因为 Phoenix 无法自动识别 HBase 中原有的表，所以需要将 HBase 中已有的做映射，才能够被 Phoenix 识别并操作。说白了就是要需要告诉 Phoenix 一声 xx 表的 xx 列是主键，xx 列的数据类型。

Phoenix映射操作：

首先创建与命名空间对应的schema，如果不创建的话，会报错"Schema does not exist schemaName=binlog (state=43M05,code=722)"

```sql
CREATE SCHEMA IF NOT EXISTS "msun";
```

- 映射视图view。视图只是供查询操作，不能修改数据，当删除视图的时候，对hbase中的源表不会有影响

```sql
CREATE VIEW "msun"."test3"(
  "ROW" varchar primary key,
  "cf1"."CREATE_TIME" varchar,
  "cf1"."ID" varchar,
  "cf1"."CONSUMER_PORT" varchar,
  "cf1"."CONSUMER_SYSTEM_NAME" varchar,
  "cf1"."INFO_CODE" varchar,
  "cf1"."INFO_NAME" varchar
) column_encoded_bytes=0;
```

映射表table，表可以查询及修改数据，当删除表的时候，hbase中的源数据表也会被删除(view无法upsert ,table可以upsert)

```sql
CREATE TABLE "msun"."test3"(
  "ROW" varchar primary key,
  "cf1"."CREATE_TIME" varchar,
  "cf1"."ID" varchar,
  "cf1"."CONSUMER_PORT" varchar,
  "cf1"."CONSUMER_SYSTEM_NAME" varchar,
  "cf1"."INFO_CODE" varchar,
  "cf1"."INFO_NAME" varchar
) column_encoded_bytes=0; //禁用列映射
```

```sql
select * from  "msun"."test3";
```

### 8.4 CDH集群配置、日志、jar包以及安装目录

- [CDH集群配置、日志、jar包以及安装目录和常用命令汇总](https://blog.csdn.net/u010886217/article/details/90045863)

#### 8.4.1 应用目录

默认可以直接敲命令行

```bash
/opt/cloudera/parcels/CDH/bin
查询
# ls
avro-tools                kite-dataset   sqoop-create-hive-table
beeline                   kudu           sqoop-eval
bigtop-detect-javahome    llama          sqoop-export
catalogd                  llamaadmin     sqoop-help
cli_mt                    load_gen       sqoop-import
cli_st                    mahout         sqoop-import-all-tables
flume-ng                  mapred         sqoop-job
hadoop                    oozie          sqoop-list-databases
hadoop-0.20               oozie-setup    sqoop-list-tables
hadoop-fuse-dfs           parquet-tools  sqoop-merge
hadoop-fuse-dfs.orig      pig            sqoop-metastore
hbase                     pyspark        sqoop-version
hbase-indexer             sentry         statestored
hbase-indexer-sentry      solrctl        whirr
hcat                      spark-shell    yarn
hdfs                      spark-submit   zookeeper-client
hive                      sqoop          zookeeper-server
hiveserver2               sqoop2         zookeeper-server-cleanup
impala-collect-minidumps  sqoop2-server  zookeeper-server-initialize
impalad                   sqoop2-tool
impala-shell              sqoop-codegen
```

zookeeper客户端命令位置

```groovy
/opt/cloudera/parcels/CDH/lib/zookeeper/bin/zkCli.sh
```

#### 8.4.2 日志目录

```bash
/var/log/下面有对应节点所开启服务的日志
其中
cloudera-scm-agent  CDH客户端日志
cloudera-scm-server CDH服务端日志
hadoop-hdfs hadoop日志目录
hadoop-mapreduce
hadoop-yarn
hbase hbase日志目录
hive hive日志目录
zookeeper  zookeeper日志目录
```

#### 8.4.3 配置文件目录

（1）各个部件配置

```bash
/etc/cloudera-scm-agent/config.ini cm agent的配置目录
/etc/cloudera-scm-server/ cm server的配置目录
 
/etc/hadoop/conf Hadoop各个组件的配置
/etc/hive/conf  hive配置文件目录
/etc/hbase/conf  hbase配置文件目录
/opt/cloudera/parcels/CDH/etc是hadoop集群以及组件的配置文件文件夹 
```

（2）服务运行时所有组件的配置文件目录

```bash
/var/run/cloudera-scm-agent/process/
查看内容
# ls
244-hdfs-NAMENODE
245-hdfs-SECONDARYNAMENODE
246-hdfs-NAMENODE-nnRpcWait
247-zookeeper-server
250-yarn-RESOURCEMANAGER
254-yarn-JOBHISTORY
255-hive-HIVESERVER2
256-hive-HIVEMETASTORE
260-hbase-MASTER
280-collect-host-statistics
283-host-inspector
291-cluster-host-inspector
296-cluster-host-inspector
302-hdfs-NAMENODE
303-hdfs-SECONDARYNAMENODE
304-hdfs-NAMENODE-nnRpcWait
305-hive-HIVEMETASTORE
308-collect-host-statistics
311-host-inspector
320-collect-host-statistics
323-host-inspector
333-collect-host-statistics
336-host-inspector
343-yarn-RESOURCEMANAGER
347-yarn-JOBHISTORY
348-hive-HIVESERVER2
349-hive-HIVEMETASTORE
359-yarn-RESOURCEMANAGER
363-yarn-JOBHISTORY
364-hive-HIVESERVER2
365-hive-HIVEMETASTORE
377-collect-host-statistics
380-host-inspector
391-collect-host-statistics
394-host-inspector
403-collect-host-statistics
406-host-inspector
415-solr-SOLR_SERVER
ccdeploy_hadoop-conf_etchadoopconf.cloudera.hdfs_2394207936436282336
ccdeploy_hadoop-conf_etchadoopconf.cloudera.yarn_-5589955826917190240
ccdeploy_hadoop-conf_etchadoopconf.cloudera.yarn_-5844902762347635403
ccdeploy_hbase-conf_etchbaseconf.cloudera.hbase_1078272019303849382
ccdeploy_hive-conf_etchiveconf.cloudera.hive_-2148401581528977976
ccdeploy_hive-conf_etchiveconf.cloudera.hive_-9144129025529935636
ccdeploy_solr-conf_etcsolrconf.cloudera.solr_4855637747225843610
ccdeploy_spark2-conf_etcspark2conf.cloudera.spark2_on_yarn_-5389810983416787264
ccdeploy_spark2-conf_etcspark2conf.cloudera.spark2_on_yarn_-8416474417781205702
```

#### 8.4.4 安装各个组件目录

```bash
/opt/cloudera/parcels/CDH
```

#### 8.4.5 jar包目录

（1）所有jar包所在目录

```bash
/opt/cloudera/parcels/CDH/jars
```

（2）各个服务组件对应的jar包

```bash
/opt/cloudera/parcels/CDH/lib/
 
例如sqoop：/opt/cloudera/parcels/CDH/lib/sqoop/lib
```

#### 8.4.6 Parcels包目录

（1）服务软件包数据(parcels)

```bash
/opt/cloudera/parcel-repo/
```

（2）服务软件包缓存数据

```groovy
/opt/cloudera/parcel-cache/
```

### 8.5 CDH内存分配参考

- [CDH内存分配参考](https://blog.csdn.net/xiaolong_4_2/article/details/81950205)

基于以下环境提供参考值： 5台内存32G、cpu8核的服务器，操作系统为centos6.8 ，总内存： 160G ，总核数： 40核 。

不同的环境可以根据自己环境的总内存以及总内核和这个参考值得出系数,那么：值= 推荐值 X 系数

- hdfs

| 参数                                 | 解释                                                         | 值        |
| :----------------------------------- | :----------------------------------------------------------- | :-------- |
| NameNode 的 Java 堆栈大小            | Java 进程堆栈内存的最大大小,改Java进程主要是：维护整个系统的的文件目录树，维护文件/目录的信息和每个文件对应的数据块列表，接受并处理用户的操作请求 | 至少1GB   |
| dfs.datanode.max.locked.memory       | 一个DataNode将要用来被做HDFS缓存的内存的最大值               | 至少256MB |
| DataNode 的 Java 堆栈大小            | Java 进程堆栈内存的最大大小,改Java进程主要是：周期性检测并向NameNode上报其管理的块信息，同时处理namenode给该Datanode的命令 | 512MB     |
| Failover Controller 的 Java 堆栈大小 | Java 进程堆栈内存的最大大小，改Java进程主要是：监控NameNode的健康状态，监控NameNode在zookeeper中的健康状态，监控之后管理NameNode的状态 | 至少256MB |
| JournalNode 的 Java 堆栈大小         | Java 进程堆栈内存的最大大小,改Java进程主要是：实现两个NameNode的数据同步，active的NameNode做变更的时候，会通知JournalNode 进程，standby的NameNode心跳获取 | 至少256MB |

- hive

| 参数                                           | 解释                                                         | 值    |
| :--------------------------------------------- | :----------------------------------------------------------- | :---- |
| Hive Metastore Server 的 Java 堆栈大小（字节） | Java 进程堆栈内存的最大大小,改Java进程主要是：为hive、impala、kudu提供元数据管理 | 1.5GB |
| HiveServer2 的 Java 堆栈大小                   | 由于我们的系统没有使用到hiveserver2,所以设置小的值，同时可以将它停止 | 1GB   |

- impala

| 参数                            | 解释                                                         | 值        |
| :------------------------------ | :----------------------------------------------------------- | :-------- |
| Catalog Server 的 Java 堆栈大小 | Java 进程堆栈内存的最大大小,改Java进程主要是:作为meta访问网关，从Hive Metastore等外部catalog中获取元数据信息，放到impala自己的catalog结构中，impalad执行ddl命令时通过catalogd由其代为执行，该更新则由statestored广播 | 最少256MB |
| Impala Daemon 内存限制          | Java 进程堆栈内存的最大大小,改Java进程主要是:作为客户端，接受客户的查询请求，生成查询计划树，把查询计划分发给其他的Impala Daemon(包括自己),被分配的Impala Daemon读写数据进行查询，并返回改客户端 | 1GB       |

- kafka

| 参数                     | 解释                                     | 值      |
| :----------------------- | :--------------------------------------- | :------ |
| Java Heap Size of Broker | kafka broker java 进程堆栈内存的最大大小 | 至少1GB |

- kudu

| 参数                                    | 解释                                                         | 值   |
| :-------------------------------------- | :----------------------------------------------------------- | :--- |
| Kudu Tablet Server Hard Memory Limit    | kudu tablet server最大能使用的内存，kudu写入数据的时候，是将数据先缓存到内存，然后保存到磁盘，如何设置过低，会影响写入的性能 | 3GB  |
| Kudu Tablet Server Block Cache Capacity | kudu tablet 块缓存的最大内存量                               | 2GB  |
| maintenance_manager_num_threads         | kudu对数据管理的时候最大显成熟                               | 4    |

- spark

| 参数                                      | 解释                                             | 值        |
| :---------------------------------------- | :----------------------------------------------- | :-------- |
| Java Heap Size of History Server in Bytes | spark history server java 进程堆栈内存的最大大小 | 至少512MB |

- yarn

| 参数                               | 解释                                  | 值    |
| :--------------------------------- | :------------------------------------ | :---- |
| JobHistory Server 的 Java 堆栈大小 | java 进程堆栈内存的最大大小           | 512MB |
| NodeManager 的 Java 堆栈           | java 进程堆栈内存的最大大小           | 512MB |
| 容器内存                           | 每个nodemanager为最大可分配的内存     | 9GB   |
| ResourceManager 的 Java 堆栈大小   | java 进程堆栈内存的最大大小           | 512MB |
| 最小容器内存                       | 单个任务可申请的最少内存量            | 1GB   |
| 容器内存增量                       | 单个任务可申请的内存的增量            | 512MB |
| 最大容器内存                       | 单个任务可申请的最大内存量            | 6GB   |
| 容器虚拟 CPU 内核                  | 每个nodemanager为最大可分配的内核数量 | 6     |
| 最小容器虚拟 CPU 内核数量          | 单个任务可申请的最小内核数量          | 1     |
| 容器虚拟 CPU 内核增量              | 单个任务申请的内核增量                | 1     |
| 最大容器虚拟 CPU 内核数量          | 单个任务可申请的最大内核数量          | 1     |

- zookeeper

| 参数                              | 解释                        | 值        |
| :-------------------------------- | :-------------------------- | :-------- |
| ZooKeeper Server 的 Java 堆栈大小 | java 进程堆栈内存的最大大小 | 至少512MB |

- Cloudera Management Service

| 参数                               | 解释 | 值        |
| :--------------------------------- | :--- | :-------- |
| Activity Monitor 的 Java 堆栈大小  |      | 至少1GB   |
| Alert Publisher 的 Java 堆栈       |      | 至少256MB |
| EventServer 的 Java 堆栈大小       |      | 至少1GB   |
| Host Monitor 的 Java 堆栈大小      |      | 至少1GB   |
| Host Monitor 的最大非 Java 内存    |      | 至少1.5GB |
| Service Monitor 的 Java 堆栈大小   |      | 至少1GB   |
| Service Monitor 的最大非 Java 内存 |      | 至少1.5GB |

## 9 问题记录

### 9.1 CDH环境HDFS权限问题

- [ CDH环境HDFS权限问题](https://blog.csdn.net/lingeio/article/details/97763315)

CDH环境下Hadoop平台最高权限用户是hdfs，属于supergroup组。默认HDFS会开启权限认证，所以操作时，需要将root用户切换到hdfs用户，否则会报错。

```bash
# Linux下默认是没有supergroup组的
# hadoop:x:994:hdfs,mapred,yarn
cat /etc/group
 
# 查看hdfs用户的组是hadoop
# hdfs:x:995:992:Hadoop HDFS:/var/lib/hadoop-hdfs:/sbin/nologin
cat /etc/passwd
```

所以，先在Linux添加supergroup组，把root用户添加到supergroup里，再同步权限到HDFS。

```bash
# Linux添加supergroup组
# supergroup:x:1003:
groupadd supergroup
 
# 将root添加到supergroup
# supergroup:x:1003:root
usermod -a -G supergroup root
 
# 同步系统权限信息到HDFS,会自动同步其他节点权限
# Refresh user to groups mapping successful for cdh-master/192.168.100.45:8020
# Refresh user to groups mapping successful for cdh-slave01/192.168.100.46:8020
su - hdfs -s /bin/bash -c "hdfs dfsadmin -refreshUserToGroupsMappings"
```

### 9.2 CDH server无法检测到agent的问题

- [CDH server无法检测到agent的问题 - 呢喃的歌声 - 博客园 (cnblogs.com)](https://www.cnblogs.com/yaohaitao/p/14106909.html)

搭建CDH集群已经挺多套了，在搭建CDH时候出现server无法检测到agent的问题大概可以这么解决:

1. IP,hostname问题这两个需要认真搭配，一旦IP hostname出现错误或者安装一半机器出问题，解决办法就是删除agent在mysql生成的元数据,具体操作如下（要分为server出问题还是agent出问题,如果都出问题就一起解决）

2. 删除Agent节点的UUID `# rm -rf /opt/cm-5.10.0/lib/cloudera-scm-agent/*` （删除agnet自动生成ID文件）

3. 清空主节点CM数据库 进入主节点的Mysql数据库，然后`drop database cm;` （删除agnet自动生成ID和数据库文件）

4. 在主节点上重新初始化CM数据库`# /opt/cm-5.10.0/share/cmf/schema/scm_prepare_database.sh mysql cm -hSERVERHOSTIP -uroot -pxxx --scm-host SERVERHOSTIP scm scm sc`

### 9.3 修改CDH的HostName和IP

- [修改CDH的HostName和IP_SunnyRivers的博客-CSDN博客](https://blog.csdn.net/Android_xue/article/details/103911264)

### 9.4 修改CM Server的元数据

- [修改CDH的HostName和IP_SunnyRivers的博客-CSDN博客](https://blog.csdn.net/Android_xue/article/details/103911264)

我当时使用的是mysql数据库，因此先登录mysql数据库

- 切换数据库

```sql
use cm;
```

- 查看几个重要字段

```sql
select host_id, host_identifier, name, ip_address  from HOSTS;
```

结果大概如下：

```sql
+---------+--------------------------------------+---------------+--------------+
| host_id | host_identifier                      | name          | ip_address   |
+---------+--------------------------------------+---------------+--------------+
|       1 | 1134ea20-6039-4ac7-b5e3-7a67d556f20e | Utility01     | 10.169.xx.xxx  |
|       2 | 50d9ad8b-d858-45ae-b727-9764eaf347d9 | slave05       | 10.169.xx.xxx |
...
...
|      20 | 59be2b8f-7103-4a40-b623-241c87402e29 | INSIDE-PHY349 | 10.169.xx.xxx  |
```

我们现在就是要把name为 INSIDE-PHY349 的节点，修改为master01

- 修改hostname

```sql
update HOSTS set name='master01' where host_id=20;
```

- 启动CM和集群即可

如果要修改ip，也是类似的

### 9.5 CDH server启动失败

- [解决 CDH6 启动 cloudera-scm-server 失败问题 - 钇钛网 (yourtion.com)](https://blog.yourtion.com/fix-cloudera-scm-server-start-failed.html)

通过命令`journalctl -xe`发现了一些端倪，提示`JAVA_HOME`找不到，但是我明明已经安装过了jdk，使用`java -version`也可以正常列出版本信息，怎么还会找不到呢？之后在一个脚本文件中找到了些答案，原来程序默认会去使用`/usr/java`下的jdk，所以解决办法执行以下两条命令即可：

```bash
mkdir -p /usr/java
ln -s /opt/java/  /usr/java/default
```

## 10 CDH集群角色和节点数规划建议

- [笨鸟的平凡之路-CDH集群角色和节点数规划建议](https://blog.csdn.net/weixin_45109718/article/details/91950033)
- [集群规划（一）角色分配](https://blog.csdn.net/xuefenxi/article/details/81563033)

### 10.1 CDH 节点数量建议

#### 10.1.1 小规模集群

一般来说，小于20个节点的都属于小规模集群，受限于集群的存储和处理能力，小规模集群不太适合用于多业务的环境。可以部署成 HBase 的集群，也可以部署成分析集群，包含 YARN、Impala 。在小规模集群中，为了最大化利用集群的存储和处理能力，节点的复用程度往往比较高。

#### 10.1.2 中等规模集群

一个中等规模的集群，集群的节点数一般在20到200左右，通常的数据存储可以规划到几百TB，适用于一个中型企业的数据平台，或者大型企业的业务部门数据平台。节点的复用程度可以降低，可以按照管理节点、主节点、工具节点和工作节点来划分。

这些节点中包含：

**1个CM管理节点**：用来安装 Cloudera Manager 和Cloudera Management Service，以对CDH进行管理。

**3个主节点**：用来安装 CDH 服务以HA 的组件。如3个ZKServer、两个 NameNode（主备）、两个ResourceManager，3个HBase Master1个Hive Metastore 、1个spark History Server之类管理角色。角色分配可以参考下图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190614093741865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTEwOTcxOA==,size_16,color_FFFFFF,t_70)

**N个工具节点**：用来部署HIVE Server2，、Hue Server、Oozie Server、Flum Agent 、Sqoop Client 、Gateway等。

**N个工作节点**：部署HDFS DataNode,YARN NodeManager,Implala Daemon,HBase Region Server。

------

#### 10.1.3 大规模集群

大规模集群的节点数量一般会在 200 以上，存储容量可以是几百TB甚至是PB级别的数据，适用于大型企业搭建的数据平台。大型集群的架构和中型集群的类似，只是主节点的数量从3个增加到5个，从而增加了主节点的可用性。剩下都是工作节点的增加。

当主节点增加到5个后，HDFS JournalNode 也从3个增加到5个，ZooKeeper Server 和HBase Master 也从3个增加到5个，Hive Metastore 由1个增加到3个。

### 10.2 CDH 节点推荐的硬件配置

业务类型不同，集群具体配置也有区别。

#### 10.2.1 实时流处理服务集群

由于性能的原因， Hadoop 实时流处理对节点内存和 CPU 有较高要求，基于 Spark Streaming 的流处理消息吞吐量可随节点数量增加而线性增长，配置可参考下图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190614094008357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTEwOTcxOA==,size_16,color_FFFFFF,t_70)

#### 10.2.2 在线分析业务集群

在线分析业务一般基于Impala等 MPP SQL 引擎，复杂的 SQL计算对内存容量有较高要求，因此需要128GB至更多的内存的硬件，推荐配置如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190614094442892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTEwOTcxOA==,size_16,color_FFFFFF,t_70)

#### 10.2.3 云存储业务集群

存储业务主要面向海量数据和文件的存储和计算，强调单节点存储容量和成本，因此配置相对廉价的SATA 硬盘，满足成本和容量的需求，推荐配置如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/2019061409453190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTEwOTcxOA==,size_16,color_FFFFFF,t_70)

### 10.3 集群规划（一）角色分配

- [Recommended Cluster Hosts and Role Distribution | 6.3.x | Cloudera Documentation](https://docs.cloudera.com/documentation/enterprise/latest/topics/cm_ig_host_allocations.html)

#### 10.3.1 文档编写目的

本文主要介绍由Cloudera Manager管理的CDH集群的角色划分。实际部署你可能还需要考虑工作负载的类型和数量，真实要部署的哪些服务，硬件资源，配置，以及其他因素。当你使用Cloudera Manager的安装向导来安装CDH时，CM会根据主机的可用资源，自动的分配角色到各台主机，边缘节点除外。你可以在向导中使用”自定义角色分配 - Customize Role Assignments”来更改这些默认划分，当然你也可以以后再使用Cloudera Manager来增加或修改角色分配。

在介绍角色划分时，我们首先来看看有哪几种主要的角色：

- 管理节点（Master Hosts）：主要用于运行Hadoop的管理进程，比如HDFS的NameNode，YARN的ResourceManager。

- 工具节点（Utility Hosts）:主要用于运行非管理进程的其他进程，比如Cloudera Manager和Hive Metastore.

- 边缘节点（Edge Hosts）：用于集群中启动作业的客户端机器，边缘节点的数量取决于工作负载的类型和数量。

- 工作节点（Worker Hosts）：主要用于运行DataNode以及其他分布式进程，比如ImpalaD。

本文会从测试/开发集群（小于10台），小规模集群（10-20台），中小规模集群（20-50台），中等规模集群（50-100台），大型集群（100-200台），超大规模集群（200-500台），巨型规模集群（500台以上）来分别讲述角色划分。以下角色划分场景都不包括Kafka，Kafka角色我们一般都会采用单独的机器部署。

#### 10.3.2 集群角色划分

##### 10.3.2.1 小于10台

一般用于测试/开发集群，我们建议至少5台机器，没有高可用。一个管理节点主要用于安装NameNode和ResourceManager，工具节点和边缘节点复用一个，主要用于安装Cloudera Manager等，剩余3-7台工作节点。

![这里写图片描述](https://img-blog.csdn.net/2018081014321684?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1ZWZlbnhp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

##### 10.3.2.2 10-20台

这是最小规模的生产系统，必须启用高可用。

我们会用2个管理节点用于安装2个NameNode，一个工具节点用于安装Cloudera Manager等，如果机器充足或者Hue/HiveServer2/Flume的负载特别高，可以考虑独立出边缘节点用于部署这些角色，否则也可以跟Cloudera Manager复用。最后还剩下7-17个工作节点

![这里写图片描述](https://img-blog.csdn.net/20180810143303116?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1ZWZlbnhp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注：根据实际情况选择是否需要单独的边缘节点。

##### 10.3.2.3 20-50台

这是中小规模的生产集群，必须启用高可用，与小规模集群角色划分差别不大。

我们会用3个管理节点用于安装NameNode和Zookeeper等，一个工具节点用于安装ClouderaManager等，如果机器充足或者Hue/HiveServer2/Flume的负载特别高，可以考虑独立出边缘节点用于部署这些角色，否则也可以跟Cloudera Manager复用。最后还剩下16-46个工作节点。

![这里写图片描述](https://img-blog.csdn.net/20180810143347810?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1ZWZlbnhp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注：根据实际情况选择是否需要单独的边缘节点。

Zookeeper和JournalNode需配置专有的数据盘

##### 10.3.2.4 50-100台

这是中等规模的生产集群，必须启用高可用。我们会用3个管理节点用于安装NameNode和Zookeeper等，一个工具节点用于安装Cloudera Manager，一个工具节点用于安装ClouderaManagement Service和Navigator等。

使用三个节点安装Hue/HiveServer2/Flume，作为边缘节点，使用两个节点安装负载均衡软件比如F5或者HAProxy并配置为KeepAlive的主主模式，该负载均衡可同时用于HiveServer2和Impala Daemon。最后还剩下42-92个工作节点。

![这里写图片描述](https://img-blog.csdn.net/20180810143600469?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1ZWZlbnhp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注：Zookeeper和JournalNode需配置专有的数据盘

##### 10.3.2.5 100-200台

属于大规模的生产集群，必须启用高可用。我们会用5个管理节点用于安装NameNode和Zookeeper等，1个工具节点用于安装Cloudera Manager，再使用4个工具节点分别安装HMS，Activity Monitor，Navigator等。

使用3个以上节点安装Hue/HiveServer2/Flume，作为边缘节点，使用2个节点安装负载均衡软件比如F5或者HAProxy并配置为KeepAlive的主主模式，该负载均衡可同时用于HiveServer2和Impala Daemon。最后还剩下85-185个工作节点。

![这里写图片描述](https://img-blog.csdn.net/20180810143611193?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1ZWZlbnhp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注：Zookeeper和JournalNode需配置专有的数据盘

Kudu Master不超过3个

Kudu Tablet Server不超过100个

##### 10.3.2.6 200-500台

属于超大规模的生产集群，必须启用高可用。我们会用7个管理节点用于安装NameNode和Zookeeper等，1个工具节点用于安装Cloudera Manager，再使用7个工具节点分别安装HMS，Activity Monitor，Navigator等。

使用3个以上节点安装Hue/HiveServer2/Flume，作为边缘节点，使用2个节点安装负载均衡软件比如F5或者HAProxy并配置为KeepAlive的主主模式，该负载均衡可同时用于HiveServer2和Impala Daemon。最后还剩下180-480个工作节点。

![这里写图片描述](https://img-blog.csdn.net/20180810143620489?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1ZWZlbnhp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注：Zookeeper和JournalNode需配置专有的数据盘

Kudu Master不超过3个

Kudu Tablet Server不超过100个

##### 10.3.2.7 500台以上

属于巨型规模的生产集群，必须启用高可用。我们会用20个管理节点用于安装NameNode和Zookeeper等，1个工具节点用于安装Cloudera Manager，再使用7个工具节点分别安装HMS，Activity Monitor，Navigator等。使用3个以上节点安装

Hue/HiveServer2/Flume，作为边缘节点，使用2个节点安装负载均衡软件比如F5或者HAProxy并配置为KeepAlive的主主模式，该负载均衡可同时用于HiveServer2和Impala Daemon。最后还剩下至少467个工作节点。

![这里写图片描述](https://img-blog.csdn.net/2018081014362819?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1ZWZlbnhp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注：这个规模的规划仅供参考，这种巨型规模的生产集群的角色划分依赖因素非常多，比如是否考虑NN和RM的联邦等

Zookeeper和JournalNode需配置专有的数据盘

Kudu Master不超过3个

Kudu Tablet Server不超过100个

## 11 CDH优化

- [CDH优化(收藏篇)_](https://blog.csdn.net/ytp552200ytp/article/details/108144251)

### 11.1 测试环境参考方案

#### 1.0 资源：集群服务安排

- 表格方便大家复制修改

| 服务名称  | 子服务                                                       | CM-24G    | ZK-Kafka(3台)-12G | DataNode(3台)-64G | NameNode1-16G | NameNode2-16G | Resourcemanager1-16G | Resourcemanager2-16G | hive-hbase-24G | hive-hbase-24G |
| :-------- | :----------------------------------------------------------- | :-------- | :---------------- | :---------------- | :------------ | :------------ | :------------------- | :------------------- | :------------- | :------------- |
| MySQL     | MySQL                                                        | √         |                   |                   |               |               |                      |                      |                |                |
| CM        | Activity Monitor Alert Publisher Event Server Host Monitor Service Monitor | √ √ √ √ √ |                   |                   |               |               |                      |                      |                |                |
| HDFS      | NameNode DataNode Failover Controller JournalNode            |           | X X X √           | X √ X X           | √ X √ X       | √ X √ X       |                      |                      |                |                |
| Yarn      | NodeManager Resourcemanager JobHisoryServer                  |           |                   | √ X X             |               |               | X √ √                | X √ √                |                |                |
| Zookeeper | Zookeeper Server                                             |           | √                 |                   |               |               |                      |                      |                |                |
| Kafka     | Kafka Broker                                                 |           | √                 |                   |               |               |                      |                      |                |                |
| Hive      | Hive Metastore Server HiveServer2 Gateway（安装对应应用服务器） |           |                   | X √ √             |               |               | X X √                | X X √                | √ X X          | √ X X          |
| Hbase     | HMaster HRegionServer Thrift Server                          |           |                   | X √ √             |               |               |                      |                      | √ X X          | √ X X          |
| Oozie     | Oozie Server                                                 |           |                   |                   |               |               | √                    |                      |                |                |
| Hue       | Hue Server Load Balancer                                     |           |                   |                   |               |               | X √                  | √ X                  |                |                |
| Spark     | History Server Gateway（安装对应应用服务器）                 |           |                   | X √               |               |               | X √                  | √ X                  |                |                |
| Flume     | Flume Agent （安装对应应用服务器）                           |           |                   |                   |               |               |                      |                      |                |                |
| Sqoop     | Sqoop（安装对应应用服务器）                                  |           |                   |                   |               |               |                      |                      |                |                |
| sorl      |                                                              |           | √                 |                   |               |               |                      |                      |                |                |

- 图

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMTIzNTg3MC8yMDIwMDgvMTIzNTg3MC0yMDIwMDgxMTE0MDgyMDkzNy05ODU2NTk1NTQucG5n?x-oss-process=image/format,png)
![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMTIzNTg3MC8yMDIwMDgvMTIzNTg3MC0yMDIwMDgxMTE0MDgyMjY1MS05MDYzNjY1NTkucG5n?x-oss-process=image/format,png)

#### 1.1 优化：Cloudera Management

> 这些服务主要是提供监控功能，目前的调整主要集中在内存放，以便有足够的资源 完成集群管理。

| 服务             | 选项           | 配置值 |
| :--------------- | :------------- | :----- |
| Activity Monitor | Java Heap Size | 2G     |
| Alert Publisher  | Java Heap Size | 2G     |
| Event Server     | Java Heap Size | 2G     |
| Host Monitor     | Java Heap Size | 4G     |
| Service Monitor  | Java Heap Size | 4G     |

#### 1.2 优化：Zookeeper

| 服务      | 选项                                  | 配置值                               |
| :-------- | :------------------------------------ | :----------------------------------- |
| Zookeeper | Java Heap Size (堆栈大小)             | 2G                                   |
| Zookeeper | maxClientCnxns (最大客户端连接数)     | 1024                                 |
| Zookeeper | dataDir (数据文件目录+数据持久化路径) | /hadoop/zookeeper (建议独立目录)     |
| Zookeeper | dataLogDir (事务日志目录)             | /hadoop/zookeeper_log (建议独立目录) |
| Zookeeper | maxSessionTimeout                     | 180000                               |

#### 1.3 优化：HDFS

| 服务        | 选项                                                     | 配置值             |
| :---------- | :------------------------------------------------------- | :----------------- |
| NameNode    | Java Heap Size (堆栈大小)10                              | 10G                |
| NameNode    | dfs.namenode.handler.count (详见3.3.2)                   | 30                 |
| NameNode    | dfs.namenode.service.handler.count (详见3.3.2)           | 30                 |
| NameNode    | fs.permissions.umask-mode (使用默认值022)                | 027(使用默认值022) |
| DataNode    | Java Heap Size (堆栈大小)                                | 8G                 |
| DataNode    | dfs.datanode.failed.volumes.tolerated (详见3.3.3)        | 0                  |
| DataNode    | dfs.datanode.balance.bandwidthPerSec (DataNode 平衡带宽) | 100M               |
| DataNode    | dfs.datanode.handler.count (服务器线程数)                | 64                 |
| DataNode    | dfs.datanode.max.transfer.threads (最大传输线程数)       | 20480              |
| JournalNode | Java Heap Size (堆栈大小)                                | 512M               |

##### 1.3.1 数据块优化

> dfs.blocksize = 128M

- 文件以块为单位进行切分存储，块通常设置的比较大（最小6M，默认128M）,根据网络带宽计算最佳值。
- 块越大，寻址越快，读取效率越高，但同时由于MapReduce任务也是以块为最小单位来处理，所以太大的块不利于于对数据的并行处理。
- 一个文件至少占用一个块（如果一个1KB文件，占用一个块，但是占用空间还是1KB）
- 我们在读取HDFS上文件的时候，NameNode会去寻找block地址，寻址时间为传输时间的1%时，则为最佳状态。
  - 目前磁盘的传输速度普遍为100MB/S
  - 如果寻址时间约为10ms，则传输时间=10ms/0.01=1000ms=1s
  - 如果传输时间为1S，传输速度为100MB/S，那么一秒钟我们就可以向HDFS传送100MB文件，设置块大小128M比较合适。
  - 如果带宽为200MB/S，那么可以将block块大小设置为256M比较合适。

##### 1.3.2 NameNode 的服务器线程的数量

- dfs.namenode.handler.count=20*log2(Cluster Size)，比如集群规模为16 ，8以2为底的对数是4，故此参数设置为80
- dfs.namenode.service.handler.count=20*log2(Cluster Size)，比如集群规模为16 ，8以2为底的对数是4，故此参数设置为80

> NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。该值需要设置为集群大小的自然对数乘以20,。

##### 1.3.3 DataNode 停止提供服务前允许失败的卷的数量

> DN多少块盘损坏后停止服务，默认为0，即一旦任何磁盘故障DN即关闭。 对盘较多的集群（例如DN有超过2块盘），磁盘故障是常态，通常可以将该值设置为1或2，避免频繁有DN下线。

#### 1.4 优化：YARN + MapReduce

| 服务              | 选项                                                         | 配置值 | 参数说明                                                     |
| :---------------- | :----------------------------------------------------------- | :----- | :----------------------------------------------------------- |
| ResourceManager   | Java Heap Size (堆栈大小)                                    | 4G     |                                                              |
| ResourceManager   | yarn.scheduler.minimum-allocation-mb (最小容器内存)          | 2G     | 给应用程序 Container 分配的最小内存                          |
| ResourceManager   | yarn.scheduler.increment-allocation-mb (容器内存增量)        | 512M   | 如果使用 Fair Scheduler，容器内存允许增量                    |
| ResourceManager   | yarn.scheduler.maximum-allocation-mb (最大容器内存)          | 32G    | 给应用程序 Container 分配的最大内存                          |
| ResourceManager   | yarn.scheduler.minimum-allocation-vcores (最小容器虚拟 CPU 内核数量) | 1      | 每个 Container 申请的最小 CPU 核数                           |
| ResourceManager   | yarn.scheduler.increment-allocation-vcores (容器虚拟 CPU 内核增量) | 1      | 如果使用 Fair Scheduler,虚拟 CPU 内核允许增量                |
| ResourceManager   | yarn.scheduler.maximum-allocation-vcores (最大容器虚拟 CPU 内核数量) | 16     | 每个 Container 申请的最大 CPU 核数                           |
| ResourceManager   | yarn.resourcemanager.recovery.enabled                        | true   | 启用后，ResourceManager 中止时在群集上运行的任何应用程序将在 ResourceManager 下次启动时恢复，**备注：**如果启用 RM-HA，则始终启用该配置。 |
| NodeManager       | Java Heap Size (堆栈大小)                                    | 4G     |                                                              |
| NodeManager       | yarn.nodemanager.resource.memory-mb                          | 10G    | 可分配给容器的物理内存数量，参照资源池内存90%左右            |
| NodeManager       | yarn.nodemanager.resource.cpu-vcores                         | 20     | 可以为容器分配的虚拟 CPU 内核的数量，参照资源池内存90%左右   |
| ApplicationMaster | yarn.app.mapreduce.am.command-opts                           | 右红   | 传递到 MapReduce ApplicationMaster 的 Java 命令行参数 "-Djava.net.preferIPv4Stack=true " |
| ApplicationMaster | yarn.app.mapreduce.am.resource.mb (ApplicationMaster 内存)   | 4G     |                                                              |
| JobHistory        | Java Heap Size (堆栈大小)                                    | 1G     |                                                              |
| MapReduce         | mapreduce.map.memory.mb （Map 任务内存）                     | 4G     | 一个MapTask可使用的资源上限。如果MapTask实际使用的资源量超过该值，则会被强制杀死。 |
| MapReduce         | mapreduce.reduce.memory.mb (Reduce 任务内存)                 | 8G     | 一个 ReduceTask 可使用的资源上限。如果 ReduceTask 实际使用的资源量超过该值，则会被强制杀死 |
| MapReduce         | mapreduce.map.cpu.vcores                                     | 2      | 每个 MapTask 可使用的最多 cpu core 数目                      |
| MapReduce         | mapreduce.reduce.cpu.vcores                                  | 4      | 每个 ReduceTask 可使用的最多 cpu core 数目                   |
| MapReduce         | mapreduce.reduce.shuffle.parallelcopies                      | 20     | 每个 Reduce 去 Map 中取数据的并行数。                        |
| MapReduce         | mapreduce.task.io.sort.mb(Shuffle 的环形缓冲区大小)          | 512M   | 当排序文件时要使用的内存缓冲总量。注意：此内存由 JVM 堆栈大小产生（也就是：总用户 JVM 堆栈 - 这些内存 = 总用户可用堆栈空间） |
| MapReduce         | mapreduce.map.sort.spill.percent                             | 80%    | 环形缓冲区溢出的阈值                                         |
| MapReduce         | mapreduce.task.timeout                                       | 10分钟 | Task 超时时间，经常需要设置的一个参数，该参数表 达的意思为：如果一个 Task 在一定时间内没有任何进 入，即不会读取新的数据，也没有输出数据，则认为 该 Task 处于 Block 状态，可能是卡住了，也许永远会 卡住，为了防止因为用户程序永远 Block 住不退出， 则强制设置了一个该超时时间。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是 ：AttemptID:attempt_12267239451721_123456_m_00 0335_0 Timed out after 600 secsContainer killed by the ApplicationMaster。 |

#### 1.5 优化：Kafka

| 服务         | 选项                       | 配置值       | 参数说明                                                     |
| :----------- | :------------------------- | :----------- | :----------------------------------------------------------- |
| Kafka Broker | Java Heap Size of Broker   | 2G           | Broker堆栈大小                                               |
| Kafka Broker | Data Directories           | 多块独立磁盘 |                                                              |
| Kafka 服务   | Maximum Message Size       | 10M          | 服务器可以接收的消息的最大大小。此属性必须与使用者使用的最大提取大小同步。否则，不守规矩的生产者可能会发布太大而无法消费的消息 |
| Kafka 服务   | Replica Maximum Fetch Size | 20M          | 副本发送给leader的获取请求中每个分区要获取的最大字节数。此值应大于message.max.bytes。 |
| Kafka 服务   | Number of Replica Fetchers | 6            | 用于复制来自领导者的消息的线程数。增大此值将增加跟随者代理中I / O并行度。 |

#### 1.6 优化：HBase

| 服务               | 选项                                 | 配置值         | 参数说明                                                     |
| :----------------- | :----------------------------------- | :------------- | :----------------------------------------------------------- |
| HBase              | Java Heap Size                       | 18G            | 客户端 Java 堆大小（字节）主要作用来缓存Table数据，但是flush时会GC，不要太大，根据集群资源，一般分配整个Hbase集群内存的70%，16->48G就可以了 |
| HBase              | hbase.client.write.buffer            | 512M           | 写入缓冲区大小，调高该值，可以减少RPC调用次数，单数会消耗更多内存，较大缓冲区需要客户端和服务器中有较大内存，因为服务器将实例化已通过的写入缓冲区并进行处理，这会降低远程过程调用 (RPC) 的数量。 |
| HBase Master       | Java Heap Size                       | 8G             | HBase Master 的 Java 堆栈大小                                |
| HBase Master       | hbase.master.handler.count           | 300            | HBase Master 中启动的 RPC 服务器实例数量。                   |
| HBase RegionServer | Java Heap Size                       | 15G            | HBase RegionServer 的 Java 堆栈大小                          |
| HBase RegionServer | hbase.regionserver.handler.count     | 50             | RegionServer 中启动的 RPC 服务器实例数量，根据集群情况，可以适当增加该值，主要决定是客户端的请求数 |
| HBase RegionServer | hbase.regionserver.metahandler.count | 30             | 用于处理 RegionServer 中的优先级请求的处理程序的数量         |
| HBase RegionServer | zookeeper.session.timeout            | 180000ms       | ZooKeeper 会话延迟（以毫秒为单位）。HBase 将此作为建议的最长会话时间传递给 ZooKeeper 仲裁 |
| HBase RegionServer | hbase.hregion.memstore.flush.size    | 1G             | 如 memstore 大小超过此值，Memstore 将刷新到磁盘。通过运行由 hbase.server.thread.wakefrequency 指定的频率的线程检查此值。 |
| HBase RegionServer | hbase.hregion.majorcompaction        | 7              | 合并周期，在合格节点下，Region下所有的HFile会进行合并，非常消耗资源，在空闲时手动触发 |
| HBase RegionServer | hbase.hregion.majorcompaction.jitter | 0.5            | 抖动比率，根据上面的合并周期，有一个抖动比率，也不靠谱，还是手动好 |
| HBase RegionServer | hbase.hstore.compactionThreshold     | 6              | 如在任意一个 HStore 中有超过此数量的 HStoreFiles，则将运行压缩以将所有 HStoreFiles 文件作为一个 HStoreFile 重新写入。（每次 memstore 刷新写入一个 HStoreFile）您可通过指定更大数量延长压缩，但压缩将运行更长时间。在压缩期间，更新无法刷新到磁盘。长时间压缩需要足够的内存，以在压缩的持续时间内记录所有更新。如太大，压缩期间客户端会超时。 |
| HBase RegionServer | hbase.client.scanner.caching         | 1000           | 内存未提供数据的情况下扫描仪下次调用时所提取的行数。较高缓存值需启用较快速度的扫描仪，但这需要更多的内存且当缓存为空时某些下一次调用会运行较长时间 |
| HBase RegionServer | hbase.hregion.max.filesize           | 50G            | HStoreFile 最大大小。如果列组的任意一个 HStoreFile 超过此值，则托管 HRegion 将分割成两个 |
| HBase Master       | hbase.master.logcleaner.plugins      | 日志清除器插件 | org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner  |
| HBase              | hbase.replication                    | false          | 禁用复制                                                     |
| HBase              | hbase.master.logcleaner.ttl          | 10min          | 保留 HLogs 的最长时间，加上如上两条解决oldWALs增长问题       |

#### 1.7 优化：Hive

| 服务           | 选项                                                         | 配置值   | 参数说明                                                     |
| :------------- | :----------------------------------------------------------- | :------- | :----------------------------------------------------------- |
| HiveServer2    | Java Heap Size                                               | 4G       |                                                              |
| Hive MetaStore | Java Heap Size                                               | 8G       |                                                              |
| Hive Gateway   | Java Heap Size                                               | 2G       |                                                              |
| HiveServer2    | hive.execution.engine                                        | Spark    | 执行引擎切换                                                 |
| HiveServer2    | hive.fetch.task.conversion                                   | more     | Fetch抓取修改为more，可以使全局查找，字段查找，limit查找等都不走计算引擎，而是直接读取表对应储存目录下的文件，大大普通查询速度 |
| Hive           | hive.exec.mode.local.auto（hive-site.xml 服务高级配置，客户端高级配置） | true     | 开启本地模式，在单台机器上处理所有的任务，对于小的数据集，执行时间可以明显被缩短 |
| Hive           | hive.exec.mode.local.auto.inputbytes.max（hive-site.xml 服务高级配置，客户端高级配置） | 50000000 | 文件不超过50M                                                |
| Hive           | hive.exec.mode.local.auto.input.files.max（hive-site.xml 服务高级配置，客户端高级配置） | 10       | 个数不超过10个                                               |
| Hive           | hive.auto.convert.join                                       | 开启     | 在join问题上，让小表放在左边 去左链接（left join）大表，这样可以有效的减少内存溢出错误发生的几率 |
| Hive           | hive.mapjoin.smalltable.filesize（hive-site.xml 服务高级配置，客户端高级配置） | 50000000 | 50M以下认为是小表                                            |
| Hive           | hive.map.aggr                                                | 开启     | 默认情况下map阶段同一个key发送给一个reduce，当一个key数据过大时就发生数据倾斜。 |
| Hive           | hive.groupby.mapaggr.checkinterval（hive-site.xml 服务高级配置，客户端高级配置） | 200000   | 在map端进行聚合操作的条目数目                                |
| Hive           | hive.groupby.skewindata（hive-site.xml 服务高级配置，客户端高级配置） | true     | 有数据倾斜时进行负载均衡，生成的查询计划会有两个MR Job，第一个MR Job会将key加随机数均匀的分布到Reduce中，做部分聚合操作（预处理），第二个MR Job在根据预处理结果还原原始key，按照Group By Key分布到Reduce中进行聚合运算，完成最终操作 |
| Hive           | hive.exec.parallel（hive-site.xml 服务高级配置，客户端高级配置） | true     | 开启并行计算                                                 |
| Hive           | hive.exec.parallel.thread.number（hive-site.xml 服务高级配置，客户端高级配置） | 16       | 同一个sql允许的最大并行度，针对集群资源适当增加              |

#### 1.8 优化：Oozie、Hue、Solr、Spark

| 服务  | 选项                                      | 配置值 | 参数说明                   |
| :---- | :---------------------------------------- | :----- | :------------------------- |
| Oozie | Java Heap Size                            | 1G     | 堆栈大小                   |
| Hue   | Java Heap Size                            | 4G     | 堆栈大小                   |
| solr  | Java Heap Size                            | 2G     | 堆栈大小                   |
| spark | Java Heap Size of History Server in Bytes | 2G     | 历史记录服务器的Java堆大小 |

### 11.2 线上集群参考方案

#### 2.0 资源：集群服务安排

- 如下黑色字体是理论计算资源使用情况，红色字体是实际使用资源情况，注意理论值已经可以良好保证集群运行。
- 表格方便大家复制修改

| 服务名称  | 子服务                                                       | CM-24G-32G | ZK-Kafka(3台)-24G-32G | DataNode(3台)-128G-128G | NameNode1-80G-128G | NameNode2-80G-128G | Resourcemanager1-24G-32G | Resourcemanager2-24G-32G | hive-hbase-24G-32G | hive-hbase-24G-32G |
| :-------- | :----------------------------------------------------------- | :--------- | :-------------------- | :---------------------- | :----------------- | :----------------- | :----------------------- | :----------------------- | :----------------- | :----------------- |
| MySQL     | MySQL                                                        | √          |                       |                         |                    |                    |                          |                          |                    |                    |
| CM        | Activity Monitor Alert Publisher Event Server Host Monitor Service Monitor | √ √ √ √ √  |                       |                         |                    |                    |                          |                          |                    |                    |
| HDFS      | NameNode DataNode Failover Controller JournalNode            |            | X X X √               | X √ X X                 | √ X √ X            | √ X √ X            |                          |                          |                    |                    |
| Yarn      | NodeManager Resourcemanager JobHisoryServer                  |            |                       | √ X X                   |                    |                    | X √ √                    | X √ √                    |                    |                    |
| Zookeeper | Zookeeper Server                                             |            | √                     |                         |                    |                    |                          |                          |                    |                    |
| Kafka     | Kafka Broker                                                 |            | √                     |                         |                    |                    |                          |                          |                    |                    |
| Hive      | Hive Metastore Server HiveServer2 Gateway（安装对应应用服务器） |            |                       | X √ √                   |                    |                    | X X √                    | X X √                    | √ X X              | √ X X              |
| Hbase     | HMaster HRegionServer Thrift Server                          |            |                       | X √ √                   |                    |                    |                          |                          | √ X X              | √ X X              |
| Oozie     | Oozie Server                                                 |            |                       |                         |                    |                    | √                        |                          |                    |                    |
| Hue       | Hue Server Load Balancer                                     |            |                       |                         |                    |                    | X √                      | √ X                      |                    |                    |
| Spark     | History Server Gateway（安装对应应用服务器）                 |            |                       | X √                     |                    |                    | X √                      | √ X                      |                    |                    |
| Flume     | Flume Agent （安装对应应用服务器）                           |            |                       |                         |                    |                    |                          |                          |                    |                    |
| Sqoop     | Sqoop（安装对应应用服务器）                                  |            |                       |                         |                    |                    |                          |                          |                    |                    |
| sorl      |                                                              |            | √                     |                         |                    |                    |                          |                          |                    |                    |

- 图

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMTIzNTg3MC8yMDIwMDgvMTIzNTg3MC0yMDIwMDgxMTE0MjU1MzUzMy03NjY0MTM0MjAucG5n?x-oss-process=image/format,png)
![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMTIzNTg3MC8yMDIwMDgvMTIzNTg3MC0yMDIwMDgxMTE0MjU1NjY0MS0yNjc4MDczMzkucG5n?x-oss-process=image/format,png)

#### 2.1 优化：Cloudera Management

> 这些服务主要是提供监控功能，目前的调整主要集中在内存，以便有足够的资源 完成集群管理。

| 服务             | 选项           | 配置值 |
| :--------------- | :------------- | :----- |
| Activity Monitor | Java Heap Size | 2G     |
| Alert Publisher  | Java Heap Size | 2G     |
| Event Server     | Java Heap Size | 2G     |
| Host Monitor     | Java Heap Size | 4G     |
| Service Monitor  | Java Heap Size | 4G     |

#### 2.2 优化：Zookeeper

| 服务      | 选项                                  | 配置值                               |
| :-------- | :------------------------------------ | :----------------------------------- |
| Zookeeper | Java Heap Size (堆栈大小)             | 4G                                   |
| Zookeeper | maxClientCnxns (最大客户端连接数)     | 1024                                 |
| Zookeeper | dataDir (数据文件目录+数据持久化路径) | /hadoop/zookeeper (建议独立目录)     |
| Zookeeper | dataLogDir (事务日志目录)             | /hadoop/zookeeper_log (建议独立目录) |
| Zookeeper | maxSessionTimeout                     | 180000                               |

#### 2.3 优化：HDFS

| 服务        | 选项                                                     | 配置值             |
| :---------- | :------------------------------------------------------- | :----------------- |
| NameNode    | Java Heap Size (堆栈大小)                                | 56G                |
| NameNode    | dfs.namenode.handler.count (详见3.3.2)                   | 80                 |
| NameNode    | dfs.namenode.service.handler.count (详见3.3.2)           | 80                 |
| NameNode    | fs.permissions.umask-mode (使用默认值022)                | 027(使用默认值022) |
| DataNode    | Java Heap Size (堆栈大小)                                | 8G                 |
| DataNode    | dfs.datanode.failed.volumes.tolerated (详见3.3.3)        | 1                  |
| DataNode    | dfs.datanode.balance.bandwidthPerSec (DataNode 平衡带宽) | 100M               |
| DataNode    | dfs.datanode.handler.count (服务器线程数)                | 64                 |
| DataNode    | dfs.datanode.max.transfer.threads (最大传输线程数)       | 20480              |
| JournalNode | Java Heap Size (堆栈大小)                                | 1G                 |

##### 2.3.1 数据块优化

> dfs.blocksize = 128M

- 文件以块为单位进行切分存储，块通常设置的比较大（最小6M，默认128M）,根据网络带宽计算最佳值。
- 块越大，寻址越快，读取效率越高，但同时由于MapReduce任务也是以块为最小单位来处理，所以太大的块不利于于对数据的并行处理。
- 一个文件至少占用一个块（如果一个1KB文件，占用一个块，但是占用空间还是1KB）
- 我们在读取HDFS上文件的时候，NameNode会去寻找block地址，寻址时间为传输时间的1%时，则为最佳状态。
  - 目前磁盘的传输速度普遍为100MB/S
  - 如果寻址时间约为10ms，则传输时间=10ms/0.01=1000ms=1s
  - 如果传输时间为1S，传输速度为100MB/S，那么一秒钟我们就可以向HDFS传送100MB文件，设置块大小128M比较合适。
  - 如果带宽为200MB/S，那么可以将block块大小设置为256M比较合适。

##### 2.3.2 NameNode 的服务器线程的数量

- dfs.namenode.handler.count=20*log2(Cluster Size)，比如集群规模为16 ，8以2为底的对数是4，故此参数设置为80
- dfs.namenode.service.handler.count=20*log2(Cluster Size)，比如集群规模为16 ，8以2为底的对数是4，故此参数设置为80

> NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。该值需要设置为集群大小的自然对数乘以20,。

##### 2.3.3 DataNode 停止提供服务前允许失败的卷的数量

> DN多少块盘损坏后停止服务，默认为0，即一旦任何磁盘故障DN即关闭。 对盘较多的集群（例如DN有超过2块盘），磁盘故障是常态，通常可以将该值设置为1或2，避免频繁有DN下线。

#### 2.4 优化：YARN + MapReduce

| 服务              | 选项                                                         | 配置值 | 参数说明                                                     |
| :---------------- | :----------------------------------------------------------- | :----- | :----------------------------------------------------------- |
| ResourceManager   | Java Heap Size (堆栈大小)                                    | 4G     |                                                              |
| ResourceManager   | yarn.scheduler.minimum-allocation-mb (最小容器内存)          | 2G     | 给应用程序 Container 分配的最小内存                          |
| ResourceManager   | yarn.scheduler.increment-allocation-mb (容器内存增量)        | 512M   | 如果使用 Fair Scheduler，容器内存允许增量                    |
| ResourceManager   | yarn.scheduler.maximum-allocation-mb (最大容器内存)          | 32G    | 给应用程序 Container 分配的最大内存                          |
| ResourceManager   | yarn.scheduler.minimum-allocation-vcores (最小容器虚拟 CPU 内核数量) | 1      | 每个 Container 申请的最小 CPU 核数                           |
| ResourceManager   | yarn.scheduler.increment-allocation-vcores (容器虚拟 CPU 内核增量) | 1      | 如果使用 Fair Scheduler,虚拟 CPU 内核允许增量                |
| ResourceManager   | yarn.scheduler.maximum-allocation-vcores (最大容器虚拟 CPU 内核数量) | 16     | 每个 Container 申请的最大 CPU 核数                           |
| ResourceManager   | yarn.resourcemanager.recovery.enabled                        | true   | 启用后，ResourceManager 中止时在群集上运行的任何应用程序将在 ResourceManager 下次启动时恢复，**备注：**如果启用 RM-HA，则始终启用该配置。 |
| NodeManager       | Java Heap Size (堆栈大小)                                    | 4G     |                                                              |
| NodeManager       | yarn.nodemanager.resource.memory-mb                          | 40G    | 可分配给容器的物理内存数量，参照资源池内存90%左右            |
| NodeManager       | yarn.nodemanager.resource.cpu-vcores                         | 32     | 可以为容器分配的虚拟 CPU 内核的数量，参照资源池内存90%左右   |
| ApplicationMaster | yarn.app.mapreduce.am.command-opts                           | 右红   | 传递到 MapReduce ApplicationMaster 的 Java 命令行参数 "-Djava.net.preferIPv4Stack=true " |
| ApplicationMaster | yarn.app.mapreduce.am.resource.mb (ApplicationMaster 内存)   | 4G     |                                                              |
| JobHistory        | Java Heap Size (堆栈大小)                                    | 2G     |                                                              |
| MapReduce         | mapreduce.map.memory.mb （Map 任务内存）                     | 4G     | 一个MapTask可使用的资源上限。如果MapTask实际使用的资源量超过该值，则会被强制杀死。 |
| MapReduce         | mapreduce.reduce.memory.mb (Reduce 任务内存)                 | 8G     | 一个 ReduceTask 可使用的资源上限。如果 ReduceTask 实际使用的资源量超过该值，则会被强制杀死 |
| MapReduce         | mapreduce.map.cpu.vcores                                     | 2      | 每个 MapTask 可使用的最多 cpu core 数目                      |
| MapReduce         | mapreduce.reduce.cpu.vcores                                  | 4      | 每个 ReduceTask 可使用的最多 cpu core 数目                   |
| MapReduce         | mapreduce.reduce.shuffle.parallelcopies                      | 20     | 每个 Reduce 去 Map 中取数据的并行数。                        |
| MapReduce         | mapreduce.task.io.sort.mb(Shuffle 的环形缓冲区大小)          | 512M   | 当排序文件时要使用的内存缓冲总量。注意：此内存由 JVM 堆栈大小产生（也就是：总用户 JVM 堆栈 - 这些内存 = 总用户可用堆栈空间） |
| MapReduce         | mapreduce.map.sort.spill.percent                             | 80%    | 环形缓冲区溢出的阈值                                         |
| MapReduce         | mapreduce.task.timeout                                       | 10分钟 | Task 超时时间，经常需要设置的一个参数，该参数表 达的意思为：如果一个 Task 在一定时间内没有任何进 入，即不会读取新的数据，也没有输出数据，则认为 该 Task 处于 Block 状态，可能是卡住了，也许永远会 卡住，为了防止因为用户程序永远 Block 住不退出， 则强制设置了一个该超时时间。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是 ：AttemptID:attempt_12267239451721_123456_m_00 0335_0 Timed out after 600 secsContainer killed by the ApplicationMaster。 |

#### 2.5 优化：Kafka

| 服务         | 选项                       | 配置值       | 参数说明                                                     |
| :----------- | :------------------------- | :----------- | :----------------------------------------------------------- |
| Kafka Broker | Java Heap Size of Broker   | 2G           | Broker堆栈大小                                               |
| Kafka Broker | Data Directories           | 多块独立磁盘 |                                                              |
| Kafka 服务   | Maximum Message Size       | 10M          | 服务器可以接收的消息的最大大小。此属性必须与使用者使用的最大提取大小同步。否则，不守规矩的生产者可能会发布太大而无法消费的消息 |
| Kafka 服务   | Replica Maximum Fetch Size | 20M          | 副本发送给leader的获取请求中每个分区要获取的最大字节数。此值应大于message.max.bytes。 |
| Kafka 服务   | Number of Replica Fetchers | 6            | 用于复制来自领导者的消息的线程数。增大此值将增加跟随者代理中I / O并行度。 |

#### 2.6 优化：HBase

| 服务               | 选项                                 | 配置值         | 参数说明                                                     |
| :----------------- | :----------------------------------- | :------------- | :----------------------------------------------------------- |
| HBase              | Java Heap Size                       | 18G            | 客户端 Java 堆大小（字节）主要作用来缓存Table数据，但是flush时会GC，不要太大，根据集群资源，一般分配整个Hbase集群内存的70%，16->48G就可以了 |
| HBase              | hbase.client.write.buffer            | 512M           | 写入缓冲区大小，调高该值，可以减少RPC调用次数，单数会消耗更多内存，较大缓冲区需要客户端和服务器中有较大内存，因为服务器将实例化已通过的写入缓冲区并进行处理，这会降低远程过程调用 (RPC) 的数量。 |
| HBase Master       | Java Heap Size                       | 8G             | HBase Master 的 Java 堆栈大小                                |
| HBase Master       | hbase.master.handler.count           | 300            | HBase Master 中启动的 RPC 服务器实例数量。                   |
| HBase RegionServer | Java Heap Size                       | 31G            | HBase RegionServer 的 Java 堆栈大小                          |
| HBase RegionServer | hbase.regionserver.handler.count     | 100            | RegionServer 中启动的 RPC 服务器实例数量，根据集群情况，可以适当增加该值，主要决定是客户端的请求数 |
| HBase RegionServer | hbase.regionserver.metahandler.count | 60             | 用于处理 RegionServer 中的优先级请求的处理程序的数量         |
| HBase RegionServer | zookeeper.session.timeout            | 180000ms       | ZooKeeper 会话延迟（以毫秒为单位）。HBase 将此作为建议的最长会话时间传递给 ZooKeeper 仲裁 |
| HBase RegionServer | hbase.hregion.memstore.flush.size    | 1G             | 如 memstore 大小超过此值，Memstore 将刷新到磁盘。通过运行由 hbase.server.thread.wakefrequency 指定的频率的线程检查此值。 |
| HBase RegionServer | hbase.hregion.majorcompaction        | 0              | 合并周期，在合格节点下，Region下所有的HFile会进行合并，非常消耗资源，在空闲时手动触发 |
| HBase RegionServer | hbase.hregion.majorcompaction.jitter | 0              | 抖动比率，根据上面的合并周期，有一个抖动比率，也不靠谱，还是手动好 |
| HBase RegionServer | hbase.hstore.compactionThreshold     | 6              | 如在任意一个 HStore 中有超过此数量的 HStoreFiles，则将运行压缩以将所有 HStoreFiles 文件作为一个 HStoreFile 重新写入。（每次 memstore 刷新写入一个 HStoreFile）您可通过指定更大数量延长压缩，但压缩将运行更长时间。在压缩期间，更新无法刷新到磁盘。长时间压缩需要足够的内存，以在压缩的持续时间内记录所有更新。如太大，压缩期间客户端会超时。 |
| HBase RegionServer | hbase.client.scanner.caching         | 1000           | 内存未提供数据的情况下扫描仪下次调用时所提取的行数。较高缓存值需启用较快速度的扫描仪，但这需要更多的内存且当缓存为空时某些下一次调用会运行较长时间 |
| HBase RegionServer | hbase.hregion.max.filesize           | 50G            | HStoreFile 最大大小。如果列组的任意一个 HStoreFile 超过此值，则托管 HRegion 将分割成两个 |
| HBase Master       | hbase.master.logcleaner.plugins      | 日志清除器插件 | org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner  |
| HBase              | hbase.replication                    | false          | 禁用复制                                                     |
| HBase              | hbase.master.logcleaner.ttl          | 10min          | 保留 HLogs 的最长时间，加上如上两条解决oldWALs增长问题       |

#### 2.7 优化：Hive

| 服务           | 选项                                                         | 配置值   | 参数说明                                                     |
| :------------- | :----------------------------------------------------------- | :------- | :----------------------------------------------------------- |
| HiveServer2    | Java Heap Size                                               | 4G       |                                                              |
| Hive MetaStore | Java Heap Size                                               | 8G       |                                                              |
| Hive Gateway   | Java Heap Size                                               | 2G       |                                                              |
| Hive           | hive.execution.engine                                        | Spark    | 执行引擎切换                                                 |
| Hive           | hive.fetch.task.conversion                                   | more     | Fetch抓取修改为more，可以使全局查找，字段查找，limit查找等都不走计算引擎，而是直接读取表对应储存目录下的文件，大大普通查询速度 |
| Hive           | hive.exec.mode.local.auto（hive-site.xml 服务高级配置，客户端高级配置） | true     | 开启本地模式，在单台机器上处理所有的任务，对于小的数据集，执行时间可以明显被缩短 |
| Hive           | hive.exec.mode.local.auto.inputbytes.max（hive-site.xml 服务高级配置，客户端高级配置） | 50000000 | 文件不超过50M                                                |
| Hive           | hive.exec.mode.local.auto.input.files.max（hive-site.xml 服务高级配置，客户端高级配置） | 10       | 个数不超过10个                                               |
| Hive           | hive.auto.convert.join                                       | 开启     | 在join问题上，让小表放在左边 去左链接（left join）大表，这样可以有效的减少内存溢出错误发生的几率 |
| Hive           | hive.mapjoin.smalltable.filesize（hive-site.xml 服务高级配置，客户端高级配置） | 50000000 | 50M以下认为是小表                                            |
| Hive           | hive.map.aggr                                                | 开启     | 默认情况下map阶段同一个key发送给一个reduce，当一个key数据过大时就发生数据倾斜。 |
| Hive           | hive.groupby.mapaggr.checkinterval（hive-site.xml 服务高级配置，客户端高级配置） | 200000   | 在map端进行聚合操作的条目数目                                |
| Hive           | hive.groupby.skewindata（hive-site.xml 服务高级配置，客户端高级配置） | true     | 有数据倾斜时进行负载均衡，生成的查询计划会有两个MR Job，第一个MR Job会将key加随机数均匀的分布到Reduce中，做部分聚合操作（预处理），第二个MR Job在根据预处理结果还原原始key，按照Group By Key分布到Reduce中进行聚合运算，完成最终操作 |
| Hive           | hive.exec.parallel（hive-site.xml 服务高级配置，客户端高级配置） | true     | 开启并行计算                                                 |
| Hive           | hive.exec.parallel.thread.number（hive-site.xml 服务高级配置，客户端高级配置） | 16       | 同一个sql允许的最大并行度，针对集群资源适当增加              |

#### 2.8 优化：Oozie、Hue、solr、spark

| 服务  | 选项                                      | 配置值 | 参数说明                   |
| :---- | :---------------------------------------- | :----- | :------------------------- |
| Oozie | Java Heap Size                            | 2G     | 堆栈大小                   |
| Hue   | Java Heap Size                            | 4G     | 堆栈大小                   |
| solr  | Java Heap Size                            | 8G     | 堆栈大小                   |
| spark | Java Heap Size of History Server in Bytes | 4G     | 历史记录服务器的Java堆大小 |
