---
title: CDH集群离线部署
date: 2022-10-26 09:38:36
permalink: /bigdata/cdh01/
categories: 
  - 大数据
tags: 
  - 大数据
---

- [CDH集群离线部署(CM6.3.1 + CDH6.3.2 + CentOS7)_](https://blog.csdn.net/qq_40856560/article/details/109007683)
- [CDH 6.3.2 离线安装部署详细教程](https://blog.csdn.net/BoomLee/article/details/119119096)
- [CDH6.3.2离线安装（附百度网盘CDH安装包）](https://zhuanlan.zhihu.com/p/366308900)
- [CM6.3.1+CDH6.3.2环境搭建](https://zhuanlan.zhihu.com/p/425449993)
- [CDH6.3.2 详细介绍及使用](https://zhuanlan.zhihu.com/p/428044940)
- [大数据之CDH（web页面部署Hadoop）](https://blog.csdn.net/yanxifaner/article/details/120210259)
- [CDH集群部署最佳实践](https://zhuanlan.zhihu.com/p/72167108)
- [CDH 6.3 大数据平台搭建 ](https://zhuanlan.zhihu.com/p/444565129)
- [CDH6.3.1集群离线部署](https://juejin.cn/post/6844904052698906631)
- [基于阿里云的CDH集群安装_Frank](https://blog.csdn.net/qq_16906867/article/details/127519913)
- [CDH简介及CDH部署、原理和使用介绍( 版本6.3.1 )_](https://blog.csdn.net/wt334502157/article/details/120290580)
- [【手册】CDH6.3.2及hadoop生态圈工具安装部署手册（附带安装包）](https://blog.csdn.net/spark9527/article/details/116757508)
- [大数据之CDH（web页面部署Hadoop）](https://blog.csdn.net/yanxifaner/article/details/120210259)
- [CDH大数据平台搭建之集群规划](https://blog.csdn.net/qq_41924766/article/details/117561341)
- [CDH安装Phoenix(cdh6.2以上版本、镜像包和安装文档)_](https://blog.csdn.net/qq_43016289/article/details/118998355)
- [ClouderaManager6.3.1+CDH6.3.2+PHOENIX-5.0.0集成部署](https://juejin.cn/post/6901675561685516295)

## 1 原生Hadoop的问题

1. 版本管理过于混乱
2. 部署过程较为繁琐,升级难度较大
3. 兼容性差
4. 安全性低

## 2 CDH和CM(Cloudera Manager）

1. CDH(Cloudera’s Distribution Including Apache Hadoop),是Hadoop众多分中的一种，由Cloudera公司维护，基于稳定版本的Apache Hadoop构建，并集成了很多补丁，可以直接用于生产环境。就是Hadoop等大数据安装包的第三方版本的集合，提供了Hadoop等大数据服务的安装包。
2. CM(Cloudera Manager)提供了一个管理和监控Hadoop等大数据服务的web界面，能让我们方便安装大数据生态圈的大部分服务。

## 3 Hadoop自动化部署和管理平台

主流的有Apache Ambari和Cloudera Manager，相对应的Hadoop的发行版为HDP和CDH。

这种自动化部署平台的功能一般如下:

1. 提供Hadoop大数据集群
2. 管理Hadoop大数据集群
3. 监控Hadoop大数据集群

PS:HDP的公司(hortonworks)已经被CDH公司(Cloudera)收购了

## 4 Cloudera Manager架构

![img](https://img-blog.csdnimg.cn/20201011090404505.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODU2NTYw,size_16,color_FFFFFF,t_70)



1. Server：负责软件安装、配置，启动和停止服务，管理服务运行的群集。**核心**

2. Agent：安装在每台主机上。负责**启动和停止进程，**配置，监控主机。

3. Management Service：由一组执行各种监控，警报和报告功能角色的服务。**图表的生成和管理**

4. Database：存储配置和监视信息。

5. Cloudera Repository：软件由Cloudera 管理分布存储库。（有点**类似Maven的中心仓库**）；在线安装（从中心仓库拉取）和离线安装（离线库）

6. Clients：是用于与服务器进行交互的接口（API和Admin Console）

## 5 CDH下载

官方下载地址:https://archive.cloudera.com

### 5.1 CM下载

https://archive.cloudera.com/cm6/6.3.1/redhat7/yum/RPMS/x86_64/

![img](https://img-blog.csdnimg.cn/20201011090420512.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODU2NTYw,size_16,color_FFFFFF,t_70)



### 5.2 CDH下载

https://archive.cloudera.com/cdh6/6.3.2/parcels/

![img](https://img-blog.csdnimg.cn/20201011090432419.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODU2NTYw,size_16,color_FFFFFF,t_70)

注意:CDH的版本一定要和CM的版本对应

## 6 环境准备

使用VMware模拟多台主机，由于主机条件有限，只演示三台机器，配置如下:

| 主机名 | 系统    | IP             | 内存 | 磁盘 |
| ------ | ------- | -------------- | ---- | ---- |
| cdh-1  | Centos7 | 192.168.100.10 | 4G   | 60G  |
| cdh-2  | Centos7 | 192.168.100.20 | 2G   | 60G  |
| cdh-3  | Centos7 | 192.168.100.30 | 2G   | 60G  |

### 6.1 修改主机名(所有节点)

```bash
hostnamectl set-hostname cdh-1
hostnamectl set-hostname cdh-2
hostnamectl set-hostname cdh-3
```

### 6.2 关闭防火墙(所有节点)

```bash
systemctl stop firewalld
systemctl disable firewalld
```

### 6.3 关闭SELinux(所有节点)

```bash
setenforce 0 #临时关闭
#永久关闭 将SELINUX= enforcing 修改为SELINUX=disabled
vi /etc/selinux/config
SELINUX=disabled 
```

PS: 可以使用

```bash
sed -i s/SELINUX=enforcing/SELINUX=disabled/g /etc/selinux/config
```

### 6.4 配置IP到主机的映射(所有节点)

```bash
vi /etc/hosts
192.168.100.10 cdh-1
192.168.100.20 cdh-2
192.168.100.30 cdh-3
```

### 6.5 配置免密码登录(cdh-1)

```bash
# 生成公钥和私钥 三次回车
ssh-keygen
# 复制公钥和私钥
ssh-copy-id cdh-1
ssh-copy-id cdh-2
ssh-copy-id cdh-3
```

### 6.6 设置用户最大可打开文件数，进程数，内存占用(所有节点)

```bash
vi /etc/security/limits.conf
```

```bash
soft    nofile   32728
hard    nofile   1024999
soft    nproc   65535
hard    noroc    unlimited
soft    memlock    unlimited
hard    memlock    unlimited
```

```bash
sysctl -p
```

### 6.7 设置swap空间(所有节点)

```bash
echo "vm.swappiness = 0" >> /etc/sysctl.conf
```

Cloudera建议将交换空间设置为0，过多的交换空间会引起GC耗时的激增。

### 6.8 关闭大页面压缩(所有节点)

```bash
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag
```

## 7 安装

将下载好的CDH包和CM的包使用sftp上传到cdh-1

### 7.1 配置本地yum

CDH的安装包都是rpm包如果使用rpm安装方式安装起来是比较复杂的，会有很多依赖问题需要解决，就需要使用yum帮助我们解决依赖问题。

![img](https://img-blog.csdnimg.cn/20201011090650551.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODU2NTYw,size_16,color_FFFFFF,t_70)

#### 7.1.1 配置centos源(cdh-1)

```bash
#挂载centos7镜像
mount /dev/cdrom /mnt/
#删除系统自带的源
rm -rf /etc/yum.repos.d/*
#新建一个本地yum源
cat >> /etc/yum.repos.d/local.repo << EOF
[centos]
name=centos
baseurl=file:///mnt
gpgcheck=0
EOF
#验证
yum repolist
```

#### 7.1.2 安装httpd服务（cdh-1）

```bash
# 安装
yum install -y httpd
# 启动
systemctl start httpd
#开机自启
systemctl enable httpd
```

http服务可以帮助我们传输文件，默认静态资源的目录为/var/www/html

#### 7.1.2 centos源配置为http方式获取（cdh-1）

```bash
#在http服务的静态资源目录创建centos目录
mkdir /var/www/html/centos
#将centos的镜像文件复制到centos目录
cp -rvf /mnt/* /var/www/html/centos/
#可以通过http访问了
http://192.168.100.10/centos/
#修改cdh-1的本地centos源的配置
vi /etc/yum.repos.d/local.repo
baseurl=http://cdh-1/centos
#取消挂载
umount /dev/cdrom /mnt
```

其他节点配置(cdh-1,cdh-2)

```bash
cat >> /etc/yum.repos.d/local.repo << EOF
[centos]
name=centos
baseurl=http://cdh-1/centos
gpgcheck=0
EOF
#验证
yum repolist
```

#### 7.1.3 配置CM源

1. 移动文件安装包文件到http服务器静态文件目录(cdh-1)

```bash
#在/var/www/html创建存放cm包的文件夹和cdh安装包的文件夹
mkdir /var/www/html/{cm,cdh}
#将cdh的安装包和cm的包移动到创建的目录
#移动cm安装包
mv cloudera-manager-* /var/www/html/cm/
mv  enterprise-debuginfo-6.3.1-1466458.el7.x86_64.rpm oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm /var/www/html/cm
#移动cdh安装包和元数据文件
mv CDH-6.3.2-1.cdh6.3.2.p0.1605554-el7.parcel manifest.json /var/www/html/cdh/
```

2. 制作CM源生成repodata文件，需要用到createrepo这个包(cdh-1)

```bash
#安装
yum install -y createrepo
#进入到cm的rpm包存放目录
cd /var/www/html/cm
# 生成repodata文件夹
createrepo .
```

3. 配置yum源（所有节点）

```bash
cat >> /etc/yum.repos.d/cm.repo << EOF
[CM]
name=cm
baseurl=http://cdh-1/cm/
gpgcheck=0
EOF
```

### 7.2 安装

![img](https://img-blog.csdnimg.cn/20201011090730501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODU2NTYw,size_16,color_FFFFFF,t_70)

#### 7.2.1 安装依赖(所有节点)

```bash
yum install -y bind-utils libxslt cyrus-sasl-plain cyrus-sasl-gssapi portmap fuse-libs /lib/lsb/init-functions httpd mod_ssl openssl-devel python-psycopg2 Mysql-python fuse
```

#### 7.2.2 安装Cloudera Manager和Cloudera Agent(cdh-1)

```bash
#安装JDK
yum install -y oracle-j2sdk1.8.x86_64
#安装cloudera-manager
yum install  -y cloudera-manager-agent cloudera-manager-daemons cloudera-manager-server cloudera-manager-server-db-2 postgresq-server
```

#### 7.2.3 安装Mariadb

```bash
#安装
yum install -y mariadb-server
#启动和开机自启
systemctl start mariadb && systemctl enable mariadb
#配置Mariadb数据库
mysql_secure_installation
```

![img](https://img-blog.csdnimg.cn/20201011090751784.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODU2NTYw,size_16,color_FFFFFF,t_70)

#### 7.2.4 初始化管理节点(cdh-1)

1. 复制mysql的jdbc驱动包到/usr/share/java目录

需要使用sftp上传jar包到cdh-1节点上

```bash
#创建/usr/share/java目录
mkdir -p /usr/share/java
#复制jar包到/usr/share/java下
cp mysql-connector-java-5.1.48.jar /usr/share/java/
#注意:需要改名为mysql-connector-java.jar
mv /usr/share/java/mysql-connector-java-5.1.48.jar /usr/share/java/mysql-connector-java.jar
```

2. 初始化数据库

```bash
/opt/cloudera/cm/schema/scm_prepare_database.sh mysql -h localhost -uroot -proot --scm-host localhost scm root root
```

#### 7.2.5 安装agent节点

只需要在chd-2和cdh-3节点上安装

```bash
#安装jdk
yum install -y oracle-j2sdk1.8.x86_64
#安装agent
yum install cloudera-manager-daemons cloudera-manager-agent -y
```

#### 7.2.6 修改配置文件(所有节点)

修改Cloudera Agent配置文件/etc/cloudera-scm-agent/config.ini，配置server_host为主节点cdh-1

```bash
#通过vi命令修改
vi /etc/cloudera-scm-agent/config.ini
server_host=cdh-1
#也可以通过sed命令修改
sed -i "s/server_host=localhost/server_host=cdh-1/g" /etc/cloudera-scm-agent/config.ini
注意:只用使用一种命令修改就行了，推荐使用sed
```

#### 7.2.7 配置JAVA_HOME(所有节点)

```bash
vi /etc/profile
```

```bash
export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera/
export PATH=$PATH:$JAVA_HOME/bin
```

### 7.3 启动

#### 7.3.1 启动Cloudera Manager(cdh-1)

在主节点启动Cloudera Manager

```bash
#启动
systemctl start cloudera-scm-server
#设置开机自启
systemctl enable cloudera-scm-server
```

#### 7.3.2 启动Cloudera Agent(所有节点)

```bash
#启动
systemctl start cloudera-scm-agent
#开机自启
systemctl enable cloudera-scm-agent
```

可以访问http://192.168.100.10:7180 用户名密码都是admin

## 8 CDH运维

### 8.1 CDH 常用指令及问题处理

- [CDH 常用指令及问题处理](https://blog.csdn.net/zhang_qings/article/details/123679017)


常用指令：

```bash
## 主节点启动server
systemctl start cloudera-scm-server

## 从节点启动agent
systemctl start cloudera-scm-agent

## 停止
systemctl stop cloudera-scm-server
systemctl stop cloudera-scm-agent
service supervisord stop
systemctl disable cloudera-scm-server

# 关闭开机启动
systemctl disable cloudera-scm-agent 

## 查看日志
tail -F /var/log/cloudera-scm-server/cloudera-scm-server.log

tail -F /var/log/cloudera-scm-agent/cloudera-scm-agent.log

# 服务端口查看
netstat -apn | grep 7180

# 测试数据库是否 正常连接
/opt/cloudera/cm/schema/scm_prepare_database.sh mysql cmf root 123456

# 跑mapreduce 测试：
sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi 1 1
```

#### 8.1.1 操作CM

（1）启动cm

```sql
systemctl start cloudera-scm-server
```

（2）重启cm

```vbscript
systemctl restart cloudera-scm-server
```

（3）停止

```vbscript
systemctl stop cloudera-scm-server
```

#### 8.1.2 查看日志

```bash
tail -f /var/log/kafka/kafka-broker-cdh1.log

tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log
```

### 8.2 端口使用整理

- [CDH6.3.2 端口使用整理](https://blog.csdn.net/Apache_Jerry/article/details/115321227)
- [CDH常用端口汇总](https://www.jianshu.com/p/6f9b9b5f6100)

| 组件      | 端口  | 说明                                                         |
| --------- | ----- | ------------------------------------------------------------ |
| Hadoop    | 50070 | HDFS WEB UI端口                                              |
|           | 2409  | NFS Gateway 服务器端口                                       |
|           | 4242  | NFS Gateway 服务器角色内执行的装载守护程序的端口号           |
|           | 8020  | 高可用的HDFS RPC端口，NameNode 运行 HDFS 协议的端口。结合 NameNode 的主机名称建立其地址 |
|           | 8022  | HDFS Daemon 可以使用的 service-rpc 地址（而非共享客户端使用的 RPC 地址）的可选端口 |
|           | 9000  | 非高可用的HDFS RPC端口                                       |
|           | 8088  | Yarn 的WEB UI 接口                                           |
|           | 8480  | JournalNode HTTP Web UI 的端口。结合 JournalNode 的主机名称建立其 HTTP 地址 |
|           | 8481  | JournalNode Web UI 侦听的基本端口。结合 JournalNode 的主机名称建立安全的 Web UI 地址 |
|           | 8485  | JournalNode 的RPC端口，结合 JournalNode 的主机名称构建其 RPC 地址 |
|           | 8019  | ZKFC端口                                                     |
|           | 9867  | 各个 DataNode 协议的端口。结合 DataNode 的主机名称建立其 IPC 端口地址 |
|           | 9868  | SecondaryNameNode HTTP 端口。如端口为 0，服务器将在自由端口启动。结合 SecondaryNameNode 的主机名称建立其 HTTP 地址 |
|           | 9869  | 安全 SecondaryNameNode Web UI 侦听的基本端口                 |
|           | 9866  | DataNode 的 XCeiver 协议的端口。结合 DataNode 的主机名称建立其地址 |
|           | 9864  | DataNode HTTP Web UI 的端口。结合 DataNode 的主机名称建立其 HTTP 地址 |
|           | 9865  | DataNode Web UI 侦听的基本端口。结合 DataNode 的主机名称建立安全的 Web UI 地址 |
|           | 9870  | DFS NameNode Web UI 侦听的基本端口。如端口为 0，服务器将在自由端口启动。结合 NameNode 的主机名称建立其 HTTP 地址 |
|           | 9871  | 安全 NameNode Web UI 侦听的基本端口                          |
|           | 10020 | historyserver端口                                            |
|           | 14000 | REST 接口可连接至 HDFS 的端口。如果已为 HttpFS 启用 TLS/SSL，则 REST 接口通过 HTTPS 服务，否则通过 HTTP 服务 |
|           | 14001 | 用于管理界面的端口                                           |
| Zookeeper | 2181  | 客户端连接zookeeper的端口                                    |
|           | 3181  | zookeeper集群内通讯使用，Leader监听此端口                    |
|           | 4181  | zookeeper端口 用于选举leader                                 |
|           | 2888  | zookeeper集群内通讯使用，Leader监听此端口                    |
|           | 3888  | zookeeper端口 用于选举leader                                 |
|           | 9010  | ZooKeeper 服务器 RMI 注册表使用的端口。需要该端口以通过 Cloudera Manager ZooKeeper 监控需要的 RMI 启用 JMX 访问。将其作为“-Dcom.sun.management.jmxremote.port”添加到 ZooKeeper 服务器 JVM 命令行。 |
| Hbase     | 60010 | Hbase的master的WEB UI端口                                    |
|           | 60030 | Hbase的regionServer的WEB UI 管理端口                         |
| Hive      | 9083  | metastore服务默认监听端口                                    |
|           | 10000 | Hive 的JDBC端口                                              |
|           | 10002 | HiveServer2 WebUI 将侦听的端口。可以将其设为 0 以禁用 WebUI  |
|           | 50111 | WebHCat Server 用于监听连接的端口                            |
| Spark     | 7077  | spark 的master与worker进行通讯的端口 standalone集群提交Application的端口 |
|           | 8080  | master的WEB UI端口 资源调度                                  |
|           | 8081  | worker的WEB UI 端口 资源调度                                 |
|           | 4040  | Driver的WEB UI 端口 任务调度                                 |
|           | 18080 | Spark History Server的WEB UI 端口                            |
| kafka     | 9092  | Kafka集群节点之间通信的RPC端口                               |
| CDH       | 5678  | Reports Manager 侦听请求的端口                               |
|           | 7178  | Navigator Metadata Server 监听请求的端口                     |
|           | 7180  | Cloudera Manager WebUI端口                                   |
|           | 7182  | Cloudera Manager Server 与 Agent 通讯端口                    |
|           | 7184  | Event Server 监听事件发布的端口                              |
|           | 7185  | Event Server 监听事件查询的端口                              |
|           | 7186  | Navigator 审核服务器监听请求的端口                           |
|           | 8083  | Reports Manager 启动调试 web 服务器的端口。请设为 -1 以禁用调试服务器 |
|           | 8084  | Event Server 的调试页面的端口。请设为 -1 以禁用调试服务器    |
|           | 8086  | Service Monitor 的调试页面的端口。请设为 -1 以禁用调试服务器 |
|           | 8087  | Activity Monitor Web UI 端口                                 |
|           | 8089  | Navigator 审核服务器启动调试 Web 服务器的端口。请设为 -1 以禁用调试服务器 |
|           | 8091  | Host Monitor 的调试页面的端口。请设为 -1 以禁用调试服务器    |
|           | 9086  | Port for Service Monitor’s HTTPS Debug page                  |
|           | 9091  | Port for Host Monitor’s HTTPS Debug page                     |
|           | 9087  | Port for Activity Monitor’s HTTPS Debug page                 |
|           | 9994  | 公开 Host Monitor 的查询 API 的端口                          |
|           | 9995  | Host Monitor 侦听代理消息的端口                              |
|           | 9996  | 公开 Service Monitor 的查询 API 的端口                       |
|           | 9997  | Service Monitor 侦听代理消息的端口                           |
|           | 9999  | Activity Monitor 侦听端口                                    |
|           | 9998  | 公开 Activity Monitor 的查询 API 的端口                      |
|           | 10101 | Alert Publisher 侦听内部 API 请求的端口                      |
|           | 10110 | The port where Telemetry Publisher listens for requests      |
|           | 10111 | The port where Telemetry Publisher starts a debug web server. Set to -1 to disable debug server |
| HUE       | 8888  | Hue WebUI 端口                                               |

### 8.3 CDH6.3.2集成Phoenix

- [CDH6.3.2集成Phoenix ](https://www.cnblogs.com/gentlescholar/p/16009526.html)
- [CDH6.3.2安装（包括Phoenix和Kylin）](https://blog.csdn.net/monster77777/article/details/109243089)

#### 8.3.1 Phoenix映射HBase中已有的表

本地安装好 Phoenix 之后，用 phoenix 的 `!talblse` 命令列出所有表，会发现 HBase 原有的表没有被列出来。而使用 Phoenix sql 的 `CREATE` 语句创建的一张新表，则可以通过 `!tables` 命令展示出来。

这是因为 Phoenix 无法自动识别 HBase 中原有的表，所以需要将 HBase 中已有的做映射，才能够被 Phoenix 识别并操作。说白了就是要需要告诉 Phoenix 一声 xx 表的 xx 列是主键，xx 列的数据类型。

Phoenix映射操作：

首先创建与命名空间对应的schema，如果不创建的话，会报错"Schema does not exist schemaName=binlog (state=43M05,code=722)"

```sql
CREATE SCHEMA IF NOT EXISTS "msun";
```

- 映射视图view。视图只是供查询操作，不能修改数据，当删除视图的时候，对hbase中的源表不会有影响

```sql
CREATE VIEW "msun"."test3"(
  "ROW" varchar primary key,
  "cf1"."CREATE_TIME" varchar,
  "cf1"."ID" varchar,
  "cf1"."CONSUMER_PORT" varchar,
  "cf1"."CONSUMER_SYSTEM_NAME" varchar,
  "cf1"."INFO_CODE" varchar,
  "cf1"."INFO_NAME" varchar
) column_encoded_bytes=0;
```

映射表table，表可以查询及修改数据，当删除表的时候，hbase中的源数据表也会被删除(view无法upsert ,table可以upsert)

```sql
CREATE TABLE "msun"."test3"(
  "ROW" varchar primary key,
  "cf1"."CREATE_TIME" varchar,
  "cf1"."ID" varchar,
  "cf1"."CONSUMER_PORT" varchar,
  "cf1"."CONSUMER_SYSTEM_NAME" varchar,
  "cf1"."INFO_CODE" varchar,
  "cf1"."INFO_NAME" varchar
) column_encoded_bytes=0; //禁用列映射
```

```sql
select * from  "msun"."test3";
```

### 8.4 CDH集群配置、日志、jar包以及安装目录

- [CDH集群配置、日志、jar包以及安装目录和常用命令汇总](https://blog.csdn.net/u010886217/article/details/90045863)

#### 8.4.1 应用目录

默认可以直接敲命令行

```bash
/opt/cloudera/parcels/CDH/bin
查询
# ls
avro-tools                kite-dataset   sqoop-create-hive-table
beeline                   kudu           sqoop-eval
bigtop-detect-javahome    llama          sqoop-export
catalogd                  llamaadmin     sqoop-help
cli_mt                    load_gen       sqoop-import
cli_st                    mahout         sqoop-import-all-tables
flume-ng                  mapred         sqoop-job
hadoop                    oozie          sqoop-list-databases
hadoop-0.20               oozie-setup    sqoop-list-tables
hadoop-fuse-dfs           parquet-tools  sqoop-merge
hadoop-fuse-dfs.orig      pig            sqoop-metastore
hbase                     pyspark        sqoop-version
hbase-indexer             sentry         statestored
hbase-indexer-sentry      solrctl        whirr
hcat                      spark-shell    yarn
hdfs                      spark-submit   zookeeper-client
hive                      sqoop          zookeeper-server
hiveserver2               sqoop2         zookeeper-server-cleanup
impala-collect-minidumps  sqoop2-server  zookeeper-server-initialize
impalad                   sqoop2-tool
impala-shell              sqoop-codegen
```

zookeeper客户端命令位置

```groovy
/opt/cloudera/parcels/CDH/lib/zookeeper/bin/zkCli.sh
```

#### 8.4.2 日志目录

```bash
/var/log/下面有对应节点所开启服务的日志
其中
cloudera-scm-agent  CDH客户端日志
cloudera-scm-server CDH服务端日志
hadoop-hdfs hadoop日志目录
hadoop-mapreduce
hadoop-yarn
hbase hbase日志目录
hive hive日志目录
zookeeper  zookeeper日志目录
```

#### 8.4.3 配置文件目录

（1）各个部件配置

```bash
/etc/cloudera-scm-agent/config.ini cm agent的配置目录
/etc/cloudera-scm-server/ cm server的配置目录
 
/etc/hadoop/conf Hadoop各个组件的配置
/etc/hive/conf  hive配置文件目录
/etc/hbase/conf  hbase配置文件目录
/opt/cloudera/parcels/CDH/etc是hadoop集群以及组件的配置文件文件夹 
```

（2）服务运行时所有组件的配置文件目录

```bash
/var/run/cloudera-scm-agent/process/
查看内容
# ls
244-hdfs-NAMENODE
245-hdfs-SECONDARYNAMENODE
246-hdfs-NAMENODE-nnRpcWait
247-zookeeper-server
250-yarn-RESOURCEMANAGER
254-yarn-JOBHISTORY
255-hive-HIVESERVER2
256-hive-HIVEMETASTORE
260-hbase-MASTER
280-collect-host-statistics
283-host-inspector
291-cluster-host-inspector
296-cluster-host-inspector
302-hdfs-NAMENODE
303-hdfs-SECONDARYNAMENODE
304-hdfs-NAMENODE-nnRpcWait
305-hive-HIVEMETASTORE
308-collect-host-statistics
311-host-inspector
320-collect-host-statistics
323-host-inspector
333-collect-host-statistics
336-host-inspector
343-yarn-RESOURCEMANAGER
347-yarn-JOBHISTORY
348-hive-HIVESERVER2
349-hive-HIVEMETASTORE
359-yarn-RESOURCEMANAGER
363-yarn-JOBHISTORY
364-hive-HIVESERVER2
365-hive-HIVEMETASTORE
377-collect-host-statistics
380-host-inspector
391-collect-host-statistics
394-host-inspector
403-collect-host-statistics
406-host-inspector
415-solr-SOLR_SERVER
ccdeploy_hadoop-conf_etchadoopconf.cloudera.hdfs_2394207936436282336
ccdeploy_hadoop-conf_etchadoopconf.cloudera.yarn_-5589955826917190240
ccdeploy_hadoop-conf_etchadoopconf.cloudera.yarn_-5844902762347635403
ccdeploy_hbase-conf_etchbaseconf.cloudera.hbase_1078272019303849382
ccdeploy_hive-conf_etchiveconf.cloudera.hive_-2148401581528977976
ccdeploy_hive-conf_etchiveconf.cloudera.hive_-9144129025529935636
ccdeploy_solr-conf_etcsolrconf.cloudera.solr_4855637747225843610
ccdeploy_spark2-conf_etcspark2conf.cloudera.spark2_on_yarn_-5389810983416787264
ccdeploy_spark2-conf_etcspark2conf.cloudera.spark2_on_yarn_-8416474417781205702
```

#### 8.4.4 安装各个组件目录

```bash
/opt/cloudera/parcels/CDH
```

#### 8.4.5 jar包目录

（1）所有jar包所在目录

```bash
/opt/cloudera/parcels/CDH/jars
```

（2）各个服务组件对应的jar包

```bash
/opt/cloudera/parcels/CDH/lib/
 
例如sqoop：/opt/cloudera/parcels/CDH/lib/sqoop/lib
```

#### 8.4.6 Parcels包目录

（1）服务软件包数据(parcels)

```bash
/opt/cloudera/parcel-repo/
```

（2）服务软件包缓存数据

```groovy
/opt/cloudera/parcel-cache/
```



## 9 问题记录

### 9.1 CDH环境HDFS权限问题

- [ CDH环境HDFS权限问题](https://blog.csdn.net/lingeio/article/details/97763315)

CDH环境下Hadoop平台最高权限用户是hdfs，属于supergroup组。默认HDFS会开启权限认证，所以操作时，需要将root用户切换到hdfs用户，否则会报错。

```bash
# Linux下默认是没有supergroup组的
# hadoop:x:994:hdfs,mapred,yarn
cat /etc/group
 
# 查看hdfs用户的组是hadoop
# hdfs:x:995:992:Hadoop HDFS:/var/lib/hadoop-hdfs:/sbin/nologin
cat /etc/passwd
```

所以，先在Linux添加supergroup组，把root用户添加到supergroup里，再同步权限到HDFS。

```bash
# Linux添加supergroup组
# supergroup:x:1003:
groupadd supergroup
 
# 将root添加到supergroup
# supergroup:x:1003:root
usermod -a -G supergroup root
 
# 同步系统权限信息到HDFS,会自动同步其他节点权限
# Refresh user to groups mapping successful for cdh-master/192.168.100.45:8020
# Refresh user to groups mapping successful for cdh-slave01/192.168.100.46:8020
su - hdfs -s /bin/bash -c "hdfs dfsadmin -refreshUserToGroupsMappings"
```

### 9.2 CDH server无法检测到agent的问题

- [CDH server无法检测到agent的问题 - 呢喃的歌声 - 博客园 (cnblogs.com)](https://www.cnblogs.com/yaohaitao/p/14106909.html)

搭建CDH集群已经挺多套了，在搭建CDH时候出现server无法检测到agent的问题大概可以这么解决:

1. IP,hostname问题这两个需要认真搭配，一旦IP hostname出现错误或者安装一半机器出问题，解决办法就是删除agent在mysql生成的元数据,具体操作如下（要分为server出问题还是agent出问题,如果都出问题就一起解决）

2. 删除Agent节点的UUID `# rm -rf /opt/cm-5.10.0/lib/cloudera-scm-agent/*` （删除agnet自动生成ID文件）

3. 清空主节点CM数据库 进入主节点的Mysql数据库，然后`drop database cm;` （删除agnet自动生成ID和数据库文件）

4. 在主节点上重新初始化CM数据库`# /opt/cm-5.10.0/share/cmf/schema/scm_prepare_database.sh mysql cm -hSERVERHOSTIP -uroot -pxxx --scm-host SERVERHOSTIP scm scm sc`

### 9.3 修改CDH的HostName和IP

- [修改CDH的HostName和IP_SunnyRivers的博客-CSDN博客](https://blog.csdn.net/Android_xue/article/details/103911264)

### 9.4 修改CM Server的元数据

- [修改CDH的HostName和IP_SunnyRivers的博客-CSDN博客](https://blog.csdn.net/Android_xue/article/details/103911264)

我当时使用的是mysql数据库，因此先登录mysql数据库

- 切换数据库

```sql
use cm;
```

- 查看几个重要字段

```sql
select host_id, host_identifier, name, ip_address  from HOSTS;
```

结果大概如下：

```sql
+---------+--------------------------------------+---------------+--------------+
| host_id | host_identifier                      | name          | ip_address   |
+---------+--------------------------------------+---------------+--------------+
|       1 | 1134ea20-6039-4ac7-b5e3-7a67d556f20e | Utility01     | 10.169.xx.xxx  |
|       2 | 50d9ad8b-d858-45ae-b727-9764eaf347d9 | slave05       | 10.169.xx.xxx |
...
...
|      20 | 59be2b8f-7103-4a40-b623-241c87402e29 | INSIDE-PHY349 | 10.169.xx.xxx  |
```

我们现在就是要把name为 INSIDE-PHY349 的节点，修改为master01

- 修改hostname

```sql
update HOSTS set name='master01' where host_id=20;
```

- 启动CM和集群即可

如果要修改ip，也是类似的

### 9.5 CDH server启动失败

- [解决 CDH6 启动 cloudera-scm-server 失败问题 - 钇钛网 (yourtion.com)](https://blog.yourtion.com/fix-cloudera-scm-server-start-failed.html)

通过命令`journalctl -xe`发现了一些端倪，提示`JAVA_HOME`找不到，但是我明明已经安装过了jdk，使用`java -version`也可以正常列出版本信息，怎么还会找不到呢？之后在一个脚本文件中找到了些答案，原来程序默认会去使用`/usr/java`下的jdk，所以解决办法执行以下两条命令即可：

```bash
mkdir -p /usr/java
ln -s /opt/java/  /usr/java/default
```

## 10 CDH集群角色和节点数规划建议

- [笨鸟的平凡之路-CDH集群角色和节点数规划建议](https://blog.csdn.net/weixin_45109718/article/details/91950033)
- [集群规划（一）角色分配](https://blog.csdn.net/xuefenxi/article/details/81563033)

### 10.1 CDH 节点数量建议

#### 10.1.1 小规模集群

一般来说，小于20个节点的都属于小规模集群，受限于集群的存储和处理能力，小规模集群不太适合用于多业务的环境。可以部署成 HBase 的集群，也可以部署成分析集群，包含 YARN、Impala 。在小规模集群中，为了最大化利用集群的存储和处理能力，节点的复用程度往往比较高。

#### 10.1.2 中等规模集群

一个中等规模的集群，集群的节点数一般在20到200左右，通常的数据存储可以规划到几百TB，适用于一个中型企业的数据平台，或者大型企业的业务部门数据平台。节点的复用程度可以降低，可以按照管理节点、主节点、工具节点和工作节点来划分。

这些节点中包含：

**1个CM管理节点**：用来安装 Cloudera Manager 和Cloudera Management Service，以对CDH进行管理。

**3个主节点**：用来安装 CDH 服务以HA 的组件。如3个ZKServer、两个 NameNode（主备）、两个ResourceManager，3个HBase Master1个Hive Metastore 、1个spark History Server之类管理角色。角色分配可以参考下图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190614093741865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTEwOTcxOA==,size_16,color_FFFFFF,t_70)

**N个工具节点**：用来部署HIVE Server2，、Hue Server、Oozie Server、Flum Agent 、Sqoop Client 、Gateway等。

**N个工作节点**：部署HDFS DataNode,YARN NodeManager,Implala Daemon,HBase Region Server。

------

#### 10.1.3 大规模集群

大规模集群的节点数量一般会在 200 以上，存储容量可以是几百TB甚至是PB级别的数据，适用于大型企业搭建的数据平台。大型集群的架构和中型集群的类似，只是主节点的数量从3个增加到5个，从而增加了主节点的可用性。剩下都是工作节点的增加。

当主节点增加到5个后，HDFS JournalNode 也从3个增加到5个，ZooKeeper Server 和HBase Master 也从3个增加到5个，Hive Metastore 由1个增加到3个。

### 10.2 CDH 节点推荐的硬件配置

业务类型不同，集群具体配置也有区别。

#### 10.2.1 实时流处理服务集群

由于性能的原因， Hadoop 实时流处理对节点内存和 CPU 有较高要求，基于 Spark Streaming 的流处理消息吞吐量可随节点数量增加而线性增长，配置可参考下图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190614094008357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTEwOTcxOA==,size_16,color_FFFFFF,t_70)

#### 10.2.2 在线分析业务集群

在线分析业务一般基于Impala等 MPP SQL 引擎，复杂的 SQL计算对内存容量有较高要求，因此需要128GB至更多的内存的硬件，推荐配置如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190614094442892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTEwOTcxOA==,size_16,color_FFFFFF,t_70)

#### 10.2.3 云存储业务集群

存储业务主要面向海量数据和文件的存储和计算，强调单节点存储容量和成本，因此配置相对廉价的SATA 硬盘，满足成本和容量的需求，推荐配置如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/2019061409453190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTEwOTcxOA==,size_16,color_FFFFFF,t_70)

### 10.3 集群规划（一）角色分配

- [Recommended Cluster Hosts and Role Distribution | 6.3.x | Cloudera Documentation](https://docs.cloudera.com/documentation/enterprise/latest/topics/cm_ig_host_allocations.html)

#### 10.3.1 文档编写目的

本文主要介绍由Cloudera Manager管理的CDH集群的角色划分。实际部署你可能还需要考虑工作负载的类型和数量，真实要部署的哪些服务，硬件资源，配置，以及其他因素。当你使用Cloudera Manager的安装向导来安装CDH时，CM会根据主机的可用资源，自动的分配角色到各台主机，边缘节点除外。你可以在向导中使用”自定义角色分配 - Customize Role Assignments”来更改这些默认划分，当然你也可以以后再使用Cloudera Manager来增加或修改角色分配。

在介绍角色划分时，我们首先来看看有哪几种主要的角色：

- 管理节点（Master Hosts）：主要用于运行Hadoop的管理进程，比如HDFS的NameNode，YARN的ResourceManager。

- 工具节点（Utility Hosts）:主要用于运行非管理进程的其他进程，比如Cloudera Manager和Hive Metastore.

- 边缘节点（Edge Hosts）：用于集群中启动作业的客户端机器，边缘节点的数量取决于工作负载的类型和数量。

- 工作节点（Worker Hosts）：主要用于运行DataNode以及其他分布式进程，比如ImpalaD。

本文会从测试/开发集群（小于10台），小规模集群（10-20台），中小规模集群（20-50台），中等规模集群（50-100台），大型集群（100-200台），超大规模集群（200-500台），巨型规模集群（500台以上）来分别讲述角色划分。以下角色划分场景都不包括Kafka，Kafka角色我们一般都会采用单独的机器部署。

#### 10.3.2 集群角色划分

##### 10.3.2.1 小于10台

一般用于测试/开发集群，我们建议至少5台机器，没有高可用。一个管理节点主要用于安装NameNode和ResourceManager，工具节点和边缘节点复用一个，主要用于安装Cloudera Manager等，剩余3-7台工作节点。

![这里写图片描述](https://img-blog.csdn.net/2018081014321684?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1ZWZlbnhp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

##### 10.3.2.2 10-20台

这是最小规模的生产系统，必须启用高可用。

我们会用2个管理节点用于安装2个NameNode，一个工具节点用于安装Cloudera Manager等，如果机器充足或者Hue/HiveServer2/Flume的负载特别高，可以考虑独立出边缘节点用于部署这些角色，否则也可以跟Cloudera Manager复用。最后还剩下7-17个工作节点

![这里写图片描述](https://img-blog.csdn.net/20180810143303116?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1ZWZlbnhp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注：根据实际情况选择是否需要单独的边缘节点。

##### 10.3.2.3 20-50台

这是中小规模的生产集群，必须启用高可用，与小规模集群角色划分差别不大。

我们会用3个管理节点用于安装NameNode和Zookeeper等，一个工具节点用于安装ClouderaManager等，如果机器充足或者Hue/HiveServer2/Flume的负载特别高，可以考虑独立出边缘节点用于部署这些角色，否则也可以跟Cloudera Manager复用。最后还剩下16-46个工作节点。

![这里写图片描述](https://img-blog.csdn.net/20180810143347810?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1ZWZlbnhp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注：根据实际情况选择是否需要单独的边缘节点。

Zookeeper和JournalNode需配置专有的数据盘

##### 10.3.2.4 50-100台

这是中等规模的生产集群，必须启用高可用。我们会用3个管理节点用于安装NameNode和Zookeeper等，一个工具节点用于安装Cloudera Manager，一个工具节点用于安装ClouderaManagement Service和Navigator等。

使用三个节点安装Hue/HiveServer2/Flume，作为边缘节点，使用两个节点安装负载均衡软件比如F5或者HAProxy并配置为KeepAlive的主主模式，该负载均衡可同时用于HiveServer2和Impala Daemon。最后还剩下42-92个工作节点。

![这里写图片描述](https://img-blog.csdn.net/20180810143600469?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1ZWZlbnhp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注：Zookeeper和JournalNode需配置专有的数据盘

##### 10.3.2.5 100-200台

属于大规模的生产集群，必须启用高可用。我们会用5个管理节点用于安装NameNode和Zookeeper等，1个工具节点用于安装Cloudera Manager，再使用4个工具节点分别安装HMS，Activity Monitor，Navigator等。

使用3个以上节点安装Hue/HiveServer2/Flume，作为边缘节点，使用2个节点安装负载均衡软件比如F5或者HAProxy并配置为KeepAlive的主主模式，该负载均衡可同时用于HiveServer2和Impala Daemon。最后还剩下85-185个工作节点。

![这里写图片描述](https://img-blog.csdn.net/20180810143611193?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1ZWZlbnhp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注：Zookeeper和JournalNode需配置专有的数据盘

Kudu Master不超过3个

Kudu Tablet Server不超过100个

##### 10.3.2.6 200-500台

属于超大规模的生产集群，必须启用高可用。我们会用7个管理节点用于安装NameNode和Zookeeper等，1个工具节点用于安装Cloudera Manager，再使用7个工具节点分别安装HMS，Activity Monitor，Navigator等。

使用3个以上节点安装Hue/HiveServer2/Flume，作为边缘节点，使用2个节点安装负载均衡软件比如F5或者HAProxy并配置为KeepAlive的主主模式，该负载均衡可同时用于HiveServer2和Impala Daemon。最后还剩下180-480个工作节点。

![这里写图片描述](https://img-blog.csdn.net/20180810143620489?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1ZWZlbnhp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注：Zookeeper和JournalNode需配置专有的数据盘

Kudu Master不超过3个

Kudu Tablet Server不超过100个

##### 10.3.2.7 500台以上

属于巨型规模的生产集群，必须启用高可用。我们会用20个管理节点用于安装NameNode和Zookeeper等，1个工具节点用于安装Cloudera Manager，再使用7个工具节点分别安装HMS，Activity Monitor，Navigator等。使用3个以上节点安装

Hue/HiveServer2/Flume，作为边缘节点，使用2个节点安装负载均衡软件比如F5或者HAProxy并配置为KeepAlive的主主模式，该负载均衡可同时用于HiveServer2和Impala Daemon。最后还剩下至少467个工作节点。

![这里写图片描述](https://img-blog.csdn.net/2018081014362819?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h1ZWZlbnhp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注：这个规模的规划仅供参考，这种巨型规模的生产集群的角色划分依赖因素非常多，比如是否考虑NN和RM的联邦等

Zookeeper和JournalNode需配置专有的数据盘

Kudu Master不超过3个

Kudu Tablet Server不超过100个
