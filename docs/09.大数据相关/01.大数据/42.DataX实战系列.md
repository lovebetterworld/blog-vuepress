---
title: DataX~MySQL到HDFS
date: 2022-11-16 16:33:17
permalink: /pages/4ee54a/
categories:
  - 大数据相关
  - 大数据
tags:
  - 
---

## 1 HDFS 到 MySQL

```json
{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "hdfsreader",
                    "parameter": {
                        "defaultFS": "hdfs://hadoop102:8020",
                        "path": "/base_province",
                        "column": [
                            "*"
                        ],
                        "fileType": "text",
                        "compress": "gzip",
                        "encoding": "UTF-8",
                        "nullFormat": "\\N",
                        "fieldDelimiter": "\t",
                    }
                },
                "writer": {
                    "name": "mysqlwriter",
                    "parameter": {
                        "username": "root",
                        "password": "******",
                        "connection": [
                            {
                                "table": [
                                    "test_province"
                                ],
                                "jdbcUrl": "jdbc:mysql://hadoop102:3306/gmall?useUnicode=true&characterEncoding=utf-8"
                            }
                        ],
                        "column": [
                            "id",
                            "name",
                            "region_id",
                            "area_code",
                            "iso_code",
                            "iso_3166_2"
                        ],
                        "writeMode": "replace"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}
```



```json
[root@slave1 mytemplate]# python /opt/datax/bin/datax.py -r hdfsreader -w mysqlwriter >> hdfsreaderTomysqlwriter.json
[root@slave1 mytemplate]# cat hdfsreaderTomysqlwriter.json
{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "hdfsreader",
                    "parameter": {
                        "column": ["*"],
                        "hadoopConfig":{                                       "dfs.client.failover.proxy.provider.mycluster":"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider",
                                                "dfs.ha.namenodes.mycluster":"nn1,nn2",
                                                "dfs.namenode.rpc-address.mycluster.nn1":"leader:8020",
                                                "dfs.namenode.rpc-address.mycluster.nn2":"slave1:8020",
                                                "dfs.nameservices":"mycluster"
                                        },
                        "defaultFS": "hdfs://mycluster",
                        "encoding": "UTF-8",
                        "fieldDelimiter": "\t",
                        "fileType": "text",
                        "path": "/datax/datas/firstStudent.csv__527c2cb7_7889_4ef3_a22b_6401e4d15064"
                    }
                },
                "writer": {
                    "name": "mysqlwriter",
                    "parameter": {
                        "column": ["stu_id","stu_age","stu_name"],
                        "connection": [
                            {
                                "jdbcUrl": "jdbc:mysql://slave1:3306/javaAndBigdata?useUnicode=true&characterEncoding=utf-8",
                                "table": ["student"]
                            }
                        ],
                        "password": "javaAndBigdata",
                        "username": "root",
                        "writeMode": "insert"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": "2"
            }
        }
    }
}
```



### 1.1 配置文件说明

#### 1.1.1 Reader参数说明

![在这里插入图片描述](https://img-blog.csdnimg.cn/4da08d7468ea414b85e4ab1d0ff4d7c9.png)

#### 1.1.2 Writer参数说明

![在这里插入图片描述](https://img-blog.csdnimg.cn/b5e007058b3b47ada021105f77d7d94b.png)

没有万能的，看具体业务场景，mysql数据为主就用第三个，采集数据为主就用第二个

- insert into，如果没有主键，插入两条相同的数据则会保存两条，如果没有主键，插入两条相同的数据则会报错。

- replace into，如何存在主键，将整条数据删除，然后再进行插入

- ON DUPLICATE KEY UPDATE 不是将整条数据删除，而是哪列的数据改变了就更新哪列的数据。

## 4 datax执行命令动态赋值（传值）

- [datax执行命令动态赋值（传值）](https://blog.csdn.net/weixin_45541247/article/details/106128472)
- [Shell脚本给Datax的job文件传参](https://blog.csdn.net/weixin_40163498/article/details/90205477)

```json
"reader":{
    "name":"mysqlreader",
    "parameter":{
        "username":"root",
        "password":"root",
        "column":[
            id
        ],
        "splitPk": "",
        "connection":[
            {
                "table":["${table}"],
                "jdbcUrl":["***:3306/test"]
            }
        ],
        "where":"${where}"
    }
}
```

通过执行命令动态赋值，采集数据：

```bash
python /datax/bin/datax.py  -p"-Dtable='test' -Dwhere='1=1'" config/2020-05-13/test.json
```

注意：这里有两个动态参数，t a b l e 和 {table}和table和{where},在执行命令赋值的时候，顺序得一致，并且单词也要一样，固定语法就是【 -D***=** 】

## DataX 附录

### A.1 DataX MysqlWriter

MysqlWriter 插件实现了写入数据到 Mysql 主库的目的表的功能。在底层实现上， MysqlWriter 通过 JDBC 连接远程 Mysql 数据库，并执行相应的 insert into ... 或者 ( replace into ...) 的 sql 语句将数据写入 Mysql，内部会分批次提交入库，需要数据库本身采用 innodb 引擎。

MysqlWriter 通过 DataX 框架获取 Reader 生成的协议数据，根据你配置的 `writeMode` 生成

- `insert into...`(当主键/唯一性索引冲突时会写不进去冲突的行)

或者

- `replace into...`(没有遇到主键/唯一性索引冲突时，与 insert into 行为一致，冲突时会用新行替换原有行所有字段) 的语句写入数据到 Mysql。出于性能考虑，采用了 `PreparedStatement + Batch`，并且设置了：`rewriteBatchedStatements=true`，将数据缓冲到线程上下文 Buffer 中，当 Buffer 累计到预定阈值时，才发起写入请求。

```
注意：目的表所在数据库必须是主库才能写入数据；整个任务至少需要具备 insert/replace into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql
```

#### 参数说明

- **jdbcUrl**

  - 描述：目的数据库的 JDBC 连接信息。作业运行时，DataX 会在你提供的 jdbcUrl 后面追加如下属性：yearIsDateType=false&zeroDateTimeBehavior=convertToNull&rewriteBatchedStatements=true

    ```
         注意：1、在一个数据库上只能配置一个 jdbcUrl 值。这与 MysqlReader 支持多个备库探测不同，因为此处不支持同一个数据库存在多个主库的情况(双主导入数据情况)
              2、jdbcUrl按照Mysql官方规范，并可以填写连接附加控制信息，比如想指定连接编码为 gbk ，则在 jdbcUrl 后面追加属性 useUnicode=true&characterEncoding=gbk。具体请参看 Mysql官方文档或者咨询对应 DBA。
    ```

  - 必选：是

  - 默认值：无

- **username**

  - 描述：目的数据库的用户名
  - 必选：是
  - 默认值：无

- **password**

  - 描述：目的数据库的密码
  - 必选：是
  - 默认值：无

- **table**

  - 描述：目的表的表名称。支持写入一个或者多个表。当配置为多张表时，必须确保所有表结构保持一致。

    ```
         注意：table 和 jdbcUrl 必须包含在 connection 配置单元中
    ```

  - 必选：是

  - 默认值：无

- **column**

  - 描述：目的表需要写入数据的字段,字段之间用英文逗号分隔。例如: "column": ["id","name","age"]。如果要依次写入全部列，使用`*`表示, 例如: `"column": ["*"]`。

    ```
      **column配置项必须指定，不能留空！**
    
         注意：1、我们强烈不推荐你这样配置，因为当你目的表字段个数、类型等有改动时，你的任务可能运行不正确或者失败
              2、 column 不能配置任何常量值
    ```

  - 必选：是

  - 默认值：否

- **session**

  - 描述: DataX在获取Mysql连接时，执行session指定的SQL语句，修改当前connection session属性
  - 必须: 否
  - 默认值: 空

- **preSql**

  - 描述：写入数据到目的表前，会先执行这里的标准语句。如果 Sql 中有你需要操作到的表名称，请使用 `@table` 表示，这样在实际执行 Sql 语句时，会对变量按照实际表名称进行替换。比如你的任务是要写入到目的端的100个同构分表(表名称为:datax_00,datax01, ... datax_98,datax_99)，并且你希望导入数据前，先对表中数据进行删除操作，那么你可以这样配置：`"preSql":["delete from 表名"]`，效果是：在执行到每个表写入数据前，会先执行对应的 delete from 对应表名称
  - 必选：否
  - 默认值：无

- **postSql**

  - 描述：写入数据到目的表后，会执行这里的标准语句。（原理同 preSql ）
  - 必选：否
  - 默认值：无

- **writeMode**

  - 描述：控制写入数据到目标表采用 `insert into` 或者 `replace into` 或者 `ON DUPLICATE KEY UPDATE` 语句
  - 必选：是
  - 所有选项：insert/replace/update
  - 默认值：insert

- **batchSize**

  - 描述：一次性批量提交的记录数大小，该值可以极大减少DataX与Mysql的网络交互次数，并提升整体吞吐量。但是该值设置过大可能会造成DataX运行进程OOM情况。
  - 必选：否
  - 默认值：1024

#### 类型转换

类似 MysqlReader ，目前 MysqlWriter 支持大部分 Mysql 类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。

下面列出 MysqlWriter 针对 Mysql 类型转换列表:

| DataX 内部类型 | Mysql 数据类型                                       |
| -------------- | ---------------------------------------------------- |
| Long           | int, tinyint, smallint, mediumint, int, bigint, year |
| Double         | float, double, decimal                               |
| String         | varchar, char, tinytext, text, mediumtext, longtext  |
| Date           | date, datetime, timestamp, time                      |
| Boolean        | bit, bool                                            |
| Bytes          | tinyblob, mediumblob, blob, longblob, varbinary      |

- `bit类型目前是未定义类型转换`

### A.2 DataX HdfsReader 插件文档

#### 1 HdfsReader说明

HdfsReader提供了读取分布式文件系统数据存储的能力。在底层实现上，HdfsReader获取分布式文件系统上文件的数据，并转换为DataX传输协议传递给Writer。

**目前HdfsReader支持的文件格式有textfile（text）、orcfile（orc）、rcfile（rc）、sequence file（seq）和普通逻辑二维表（csv）类型格式的文件，且文件内容存放的必须是一张逻辑意义上的二维表。**

**HdfsReader需要Jdk1.7及以上版本的支持。**

#### 2 功能与限制

HdfsReader实现了从Hadoop分布式文件系统Hdfs中读取文件数据并转为DataX协议的功能。textfile是Hive建表时默认使用的存储格式，数据不做压缩，本质上textfile就是以文本的形式将数据存放在hdfs中，对于DataX而言，HdfsReader实现上类比TxtFileReader，有诸多相似之处。orcfile，它的全名是Optimized Row Columnar file，是对RCFile做了优化。据官方文档介绍，这种文件格式可以提供一种高效的方法来存储Hive数据。HdfsReader利用Hive提供的OrcSerde类，读取解析orcfile文件的数据。目前HdfsReader支持的功能如下：

1. 支持textfile、orcfile、rcfile、sequence file和csv格式的文件，且要求文件内容存放的是一张逻辑意义上的二维表。
2. 支持多种类型数据读取(使用String表示)，支持列裁剪，支持列常量
3. 支持递归读取、支持正则表达式（"*"和"?"）。
4. 支持orcfile数据压缩，目前支持SNAPPY，ZLIB两种压缩方式。
5. 多个File可以支持并发读取。
6. 支持sequence file数据压缩，目前支持lzo压缩方式。
7. csv类型支持压缩格式有：gzip、bz2、zip、lzo、lzo_deflate、snappy。
8. 目前插件中Hive版本为1.1.1，Hadoop版本为2.7.1（Apache［为适配JDK1.7］,在Hadoop 2.5.0, Hadoop 2.6.0 和Hive 1.2.0测试环境中写入正常；其它版本需后期进一步测试；
9. 支持kerberos认证（注意：如果用户需要进行kerberos认证，那么用户使用的Hadoop集群版本需要和hdfsreader的Hadoop版本保持一致，如果高于hdfsreader的Hadoop版本，不保证kerberos认证有效）

我们暂时不能做到：

1. 单个File支持多线程并发读取，这里涉及到单个File内部切分算法。二期考虑支持。
2. 目前还不支持hdfs HA;

#### 3 功能说明

##### 3.1 配置样例

```
{
    "job": {
        "setting": {
            "speed": {
                "channel": 3
            }
        },
        "content": [
            {
                "reader": {
                    "name": "hdfsreader",
                    "parameter": {
                        "path": "/user/hive/warehouse/mytable01/*",
                        "defaultFS": "hdfs://xxx:port",
                        "column": [
                               {
                                "index": 0,
                                "type": "long"
                               },
                               {
                                "index": 1,
                                "type": "boolean"
                               },
                               {
                                "type": "string",
                                "value": "hello"
                               },
                               {
                                "index": 2,
                                "type": "double"
                               }
                        ],
                        "fileType": "orc",
                        "encoding": "UTF-8",
                        "fieldDelimiter": ","
                    }

                },
                "writer": {
                    "name": "streamwriter",
                    "parameter": {
                        "print": true
                    }
                }
            }
        ]
    }
}
```

##### 3.2 参数说明（各个配置项值前后不允许有空格）

- **path**

  - 描述：要读取的文件路径，如果要读取多个文件，可以使用正则表达式"*"，注意这里可以支持填写多个路径。。

    当指定单个Hdfs文件，HdfsReader暂时只能使用单线程进行数据抽取。二期考虑在非压缩文件情况下针对单个File可以进行多线程并发读取。

    当指定多个Hdfs文件，HdfsReader支持使用多线程进行数据抽取。线程并发数通过通道数指定。

    当指定通配符，HdfsReader尝试遍历出多个文件信息。例如: 指定/*代表读取/目录下所有的文件，指定/bazhen/\*代表读取bazhen目录下游所有的文件。HdfsReader目前只支持"*"和"?"作为文件通配符。

    **特别需要注意的是，DataX会将一个作业下同步的所有的文件视作同一张数据表。用户必须自己保证所有的File能够适配同一套schema信息。并且提供给DataX权限可读。**

  - 必选：是

  - 默认值：无

- **defaultFS**

  - 描述：Hadoop hdfs文件系统namenode节点地址。

    **目前HdfsReader已经支持Kerberos认证，如果需要权限认证，则需要用户配置kerberos参数，见下面**

  - 必选：是

  - 默认值：无

- **fileType**

  - 描述：文件的类型，目前只支持用户配置为"text"、"orc"、"rc"、"seq"、"csv"。

    text表示textfile文件格式

    orc表示orcfile文件格式

    rc表示rcfile文件格式

    seq表示sequence file文件格式

    csv表示普通hdfs文件格式（逻辑二维表）

    **特别需要注意的是，HdfsReader能够自动识别文件是orcfile、textfile或者还是其它类型的文件，但该项是必填项，HdfsReader则会只读取用户配置的类型的文件，忽略路径下其他格式的文件**

    **另外需要注意的是，由于textfile和orcfile是两种完全不同的文件格式，所以HdfsReader对这两种文件的解析方式也存在差异，这种差异导致hive支持的复杂复合类型(比如map,array,struct,union)在转换为DataX支持的String类型时，转换的结果格式略有差异，比如以map类型为例：**

    orcfile map类型经hdfsreader解析转换成datax支持的string类型后，结果为"{job=80, team=60, person=70}"

    textfile map类型经hdfsreader解析转换成datax支持的string类型后，结果为"job:80,team:60,person:70"

    从上面的转换结果可以看出，数据本身没有变化，但是表示的格式略有差异，所以如果用户配置的文件路径中要同步的字段在Hive中是复合类型的话，建议配置统一的文件格式。

    **如果需要统一复合类型解析出来的格式，我们建议用户在hive客户端将textfile格式的表导成orcfile格式的表**

  - 必选：是

  - 默认值：无

- **column**

  - 描述：读取字段列表，type指定源数据的类型，index指定当前列来自于文本第几列(以0开始)，value指定当前类型为常量，不从源头文件读取数据，而是根据value值自动生成对应的列。

    默认情况下，用户可以全部按照String类型读取数据，配置如下：

    ```
     	"column": ["*"]
    ```

    用户可以指定Column字段信息，配置如下：

    ```json
    { "type": "long", "index": 0 //从本地文件文本第一列获取int字段 }, { "type": "string", "value": "alibaba" //HdfsReader内部生成alibaba的字符串字段作为当前字段 }
    ```

```
对于用户指定Column信息，type必须填写，index/value必须选择其一。
* 必选：是 <br />
* 默认值：全部按照string类型读取 <br />
```

- **fieldDelimiter**

  - 描述：读取的字段分隔符

  **另外需要注意的是，HdfsReader在读取textfile数据时，需要指定字段分割符，如果不指定默认为','，HdfsReader在读取orcfile时，用户无需指定字段分割符**

  - 必选：否
  - 默认值：,

- **encoding**

  - 描述：读取文件的编码配置。
  - 必选：否
  - 默认值：utf-8

- **nullFormat**

  - 描述：文本文件中无法使用标准字符串定义null(空指针)，DataX提供nullFormat定义哪些字符串可以表示为null。

    例如如果用户配置: nullFormat:"\N"，那么如果源头数据是"\N"，DataX视作null字段。

  - 必选：否

  - 默认值：无

- **haveKerberos**

  - 描述：是否有Kerberos认证，默认false

    例如如果用户配置true，则配置项kerberosKeytabFilePath，kerberosPrincipal为必填。

  - 必选：haveKerberos 为true必选

  - 默认值：false

- **kerberosKeytabFilePath**

  - 描述：Kerberos认证 keytab文件路径，绝对路径
  - 必选：否
  - 默认值：无

- **kerberosPrincipal**

  - 描述：Kerberos认证Principal名，如xxxx/[hadoopclient@xxx.xxx](mailto:hadoopclient@xxx.xxx)
  - 必选：haveKerberos 为true必选
  - 默认值：无

- **compress**

  - 描述：当fileType（文件类型）为csv下的文件压缩方式，目前仅支持 gzip、bz2、zip、lzo、lzo_deflate、hadoop-snappy、framing-snappy压缩；**值得注意的是，lzo存在两种压缩格式：lzo和lzo_deflate，用户在配置的时候需要留心，不要配错了；另外，由于snappy目前没有统一的stream format，datax目前只支持最主流的两种：hadoop-snappy（hadoop上的snappy stream format）和framing-snappy（google建议的snappy stream format）**;orc文件类型下无需填写。
  - 必选：否
  - 默认值：无

- **hadoopConfig**

  - 描述：hadoopConfig里可以配置与Hadoop相关的一些高级参数，比如HA的配置。

    ```
     "hadoopConfig":{
             "dfs.nameservices": "testDfs",
             "dfs.ha.namenodes.testDfs": "namenode1,namenode2",
             "dfs.namenode.rpc-address.aliDfs.namenode1": "",
             "dfs.namenode.rpc-address.aliDfs.namenode2": "",
             "dfs.client.failover.proxy.provider.testDfs": "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"
     }
    ```

  - 必选：否

  - 默认值：无

- **csvReaderConfig**

  - 描述：读取CSV类型文件参数配置，Map类型。读取CSV类型文件使用的CsvReader进行读取，会有很多配置，不配置则使用默认值。
  - 必选：否
  - 默认值：无

常见配置：

```
"csvReaderConfig":{
        "safetySwitch": false,
        "skipEmptyRecords": false,
        "useTextQualifier": false
}
```

所有配置项及默认值,配置时 csvReaderConfig 的map中请**严格按照以下字段名字进行配置**：

```
boolean caseSensitive = true;
char textQualifier = 34;
boolean trimWhitespace = true;
boolean useTextQualifier = true;//是否使用csv转义字符
char delimiter = 44;//分隔符
char recordDelimiter = 0;
char comment = 35;
boolean useComments = false;
int escapeMode = 1;
boolean safetySwitch = true;//单列长度是否限制100000字符
boolean skipEmptyRecords = true;//是否跳过空行
boolean captureRawRecord = true;
```

##### 3.3 类型转换

由于textfile和orcfile文件表的元数据信息由Hive维护并存放在Hive自己维护的数据库（如mysql）中，目前HdfsReader不支持对Hive元数

据数据库进行访问查询，因此用户在进行类型转换的时候，必须指定数据类型，如果用户配置的column为"*"，则所有column默认转换为

string类型。HdfsReader提供了类型转换的建议表如下：

| DataX 内部类型 | Hive表 数据类型                                   |
| -------------- | ------------------------------------------------- |
| Long           | TINYINT,SMALLINT,INT,BIGINT                       |
| Double         | FLOAT,DOUBLE                                      |
| String         | String,CHAR,VARCHAR,STRUCT,MAP,ARRAY,UNION,BINARY |
| Boolean        | BOOLEAN                                           |
| Date           | Date,TIMESTAMP                                    |

其中：

- Long是指Hdfs文件文本中使用整形的字符串表示形式，例如"123456789"。
- Double是指Hdfs文件文本中使用Double的字符串表示形式，例如"3.1415"。
- Boolean是指Hdfs文件文本中使用Boolean的字符串表示形式，例如"true"、"false"。不区分大小写。
- Date是指Hdfs文件文本中使用Date的字符串表示形式，例如"2014-12-31"。

特别提醒：

- Hive支持的数据类型TIMESTAMP可以精确到纳秒级别，所以textfile、orcfile中TIMESTAMP存放的数据类似于"2015-08-21 22:40:47.397898389"，如果转换的类型配置为DataX的Date，转换之后会导致纳秒部分丢失，所以如果需要保留纳秒部分的数据，请配置转换类型为DataX的String类型。

##### 3.4 按分区读取

Hive在建表的时候，可以指定分区partition，例如创建分区partition(day="20150820",hour="09")，对应的hdfs文件系统中，相应的表的目录下则会多出/20150820和/09两个目录，且/20150820是/09的父目录。了解了分区都会列成相应的目录结构，在按照某个分区读取某个表所有数据时，则只需配置好json中path的值即可。

比如需要读取表名叫mytable01下分区day为20150820这一天的所有数据，则配置如下：

```
"path": "/user/hive/warehouse/mytable01/20150820/*"
```

### A.3 HdfsWriter

#### 1 快速介绍

HdfsWriter提供向HDFS文件系统指定路径中写入TEXTFile文件和ORCFile文件,文件内容可与hive中表关联。

#### 2 功能与限制

- (1)、目前HdfsWriter仅支持textfile和orcfile两种格式的文件，且文件内容存放的必须是一张逻辑意义上的二维表;
- (2)、由于HDFS是文件系统，不存在schema的概念，因此不支持对部分列写入;
- (3)、目前仅支持与以下Hive数据类型： 数值型：TINYINT,SMALLINT,INT,BIGINT,FLOAT,DOUBLE 字符串类型：STRING,VARCHAR,CHAR 布尔类型：BOOLEAN 时间类型：DATE,TIMESTAMP **目前不支持：decimal、binary、arrays、maps、structs、union类型**;
- (4)、对于Hive分区表目前仅支持一次写入单个分区;
- (5)、对于textfile需用户保证写入hdfs文件的分隔符**与在Hive上创建表时的分隔符一致**,从而实现写入hdfs数据与Hive表字段关联;
- (6)、HdfsWriter实现过程是：首先根据用户指定的path，创建一个hdfs文件系统上不存在的临时目录，创建规则：path_随机；然后将读取的文件写入这个临时目录；全部写入后再将这个临时目录下的文件移动到用户指定目录（在创建文件时保证文件名不重复）; 最后删除临时目录。如果在中间过程发生网络中断等情况造成无法与hdfs建立连接，需要用户手动删除已经写入的文件和临时目录。
- (7)、目前插件中Hive版本为1.1.1，Hadoop版本为2.7.1（Apache［为适配JDK1.7］,在Hadoop 2.5.0, Hadoop 2.6.0 和Hive 1.2.0测试环境中写入正常；其它版本需后期进一步测试；
- (8)、目前HdfsWriter支持Kerberos认证（注意：如果用户需要进行kerberos认证，那么用户使用的Hadoop集群版本需要和hdfsreader的Hadoop版本保持一致，如果高于hdfsreader的Hadoop版本，不保证kerberos认证有效）

#### 3 功能说明

##### 3.1 配置样例

```
{
    "setting": {},
    "job": {
        "setting": {
            "speed": {
                "channel": 2
            }
        },
        "content": [
            {
                "reader": {
                    "name": "txtfilereader",
                    "parameter": {
                        "path": ["/Users/shf/workplace/txtWorkplace/job/dataorcfull.txt"],
                        "encoding": "UTF-8",
                        "column": [
                            {
                                "index": 0,
                                "type": "long"
                            },
                            {
                                "index": 1,
                                "type": "long"
                            },
                            {
                                "index": 2,
                                "type": "long"
                            },
                            {
                                "index": 3,
                                "type": "long"
                            },
                            {
                                "index": 4,
                                "type": "DOUBLE"
                            },
                            {
                                "index": 5,
                                "type": "DOUBLE"
                            },
                            {
                                "index": 6,
                                "type": "STRING"
                            },
                            {
                                "index": 7,
                                "type": "STRING"
                            },
                            {
                                "index": 8,
                                "type": "STRING"
                            },
                            {
                                "index": 9,
                                "type": "BOOLEAN"
                            },
                            {
                                "index": 10,
                                "type": "date"
                            },
                            {
                                "index": 11,
                                "type": "date"
                            }
                        ],
                        "fieldDelimiter": "\t"
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "defaultFS": "hdfs://xxx:port",
                        "fileType": "orc",
                        "path": "/user/hive/warehouse/writerorc.db/orcfull",
                        "fileName": "xxxx",
                        "column": [
                            {
                                "name": "col1",
                                "type": "TINYINT"
                            },
                            {
                                "name": "col2",
                                "type": "SMALLINT"
                            },
                            {
                                "name": "col3",
                                "type": "INT"
                            },
                            {
                                "name": "col4",
                                "type": "BIGINT"
                            },
                            {
                                "name": "col5",
                                "type": "FLOAT"
                            },
                            {
                                "name": "col6",
                                "type": "DOUBLE"
                            },
                            {
                                "name": "col7",
                                "type": "STRING"
                            },
                            {
                                "name": "col8",
                                "type": "VARCHAR"
                            },
                            {
                                "name": "col9",
                                "type": "CHAR"
                            },
                            {
                                "name": "col10",
                                "type": "BOOLEAN"
                            },
                            {
                                "name": "col11",
                                "type": "date"
                            },
                            {
                                "name": "col12",
                                "type": "TIMESTAMP"
                            }
                        ],
                        "writeMode": "append",
                        "fieldDelimiter": "\t",
                        "compress":"NONE"
                    }
                }
            }
        ]
    }
}
```

##### 3.2 参数说明

- **defaultFS**

  - 描述：Hadoop hdfs文件系统namenode节点地址。格式：hdfs://ip:端口；例如：hdfs://127.0.0.1:9000
  - 必选：是
  - 默认值：无

- **fileType**

  - 描述：文件的类型，目前只支持用户配置为"text"或"orc"。

    text表示textfile文件格式

    orc表示orcfile文件格式

  - 必选：是

  - 默认值：无

- **path**

  - 描述：存储到Hadoop hdfs文件系统的路径信息，HdfsWriter会根据并发配置在Path目录下写入多个文件。为与hive表关联，请填写hive表在hdfs上的存储路径。例：Hive上设置的数据仓库的存储路径为：/user/hive/warehouse/ ，已建立数据库：test，表：hello；则对应的存储路径为：/user/hive/warehouse/test.db/hello
  - 必选：是
  - 默认值：无

- **fileName**

  - 描述：HdfsWriter写入时的文件名，实际执行时会在该文件名后添加随机的后缀作为每个线程写入实际文件名。
  - 必选：是
  - 默认值：无

- **column**

  - 描述：写入数据的字段，不支持对部分列写入。为与hive中表关联，需要指定表中所有字段名和字段类型，其中：name指定字段名，type指定字段类型。

    用户可以指定Column字段信息，配置如下：

    ```
     "column":
              [
                         {
                             "name": "userName",
                             "type": "string"
                         },
                         {
                             "name": "age",
                             "type": "long"
                         }
              ]
    ```

  - 必选：是

  - 默认值：无

- **writeMode**

  - 描述：hdfswriter写入前数据清理处理模式：
    - append，写入前不做任何处理，DataX hdfswriter直接使用filename写入，并保证文件名不冲突。
    - nonConflict，如果目录下有fileName前缀的文件，直接报错。
    - truncate，如果目录下有fileName前缀的文件，先删除后写入。
  - 必选：是
  - 默认值：无

- **fieldDelimiter**

  - 描述：hdfswriter写入时的字段分隔符,**需要用户保证与创建的Hive表的字段分隔符一致，否则无法在Hive表中查到数据**
  - 必选：是
  - 默认值：无

- **compress**

  - 描述：hdfs文件压缩类型，默认不填写意味着没有压缩。其中：text类型文件支持压缩类型有gzip、bzip2;orc类型文件支持的压缩类型有NONE、SNAPPY（需要用户安装SnappyCodec）。
  - 必选：否
  - 默认值：无压缩

- **hadoopConfig**

  - 描述：hadoopConfig里可以配置与Hadoop相关的一些高级参数，比如HA的配置。

    ```
     "hadoopConfig":{
             "dfs.nameservices": "testDfs",
             "dfs.ha.namenodes.testDfs": "namenode1,namenode2",
             "dfs.namenode.rpc-address.aliDfs.namenode1": "",
             "dfs.namenode.rpc-address.aliDfs.namenode2": "",
             "dfs.client.failover.proxy.provider.testDfs": "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"
     }
    ```

  - 必选：否

  - 默认值：无

- **encoding**

  - 描述：写文件的编码配置。
  - 必选：否
  - 默认值：utf-8，**慎重修改**

- **haveKerberos**

  - 描述：是否有Kerberos认证，默认false

    例如如果用户配置true，则配置项kerberosKeytabFilePath，kerberosPrincipal为必填。

  - 必选：haveKerberos 为true必选

  - 默认值：false

- **kerberosKeytabFilePath**

  - 描述：Kerberos认证 keytab文件路径，绝对路径
  - 必选：否
  - 默认值：无

- **kerberosPrincipal**

  - 描述：Kerberos认证Principal名，如xxxx/[hadoopclient@xxx.xxx](mailto:hadoopclient@xxx.xxx)
  - 必选：haveKerberos 为true必选
  - 默认值：无

##### 3.3 类型转换

目前 HdfsWriter 支持大部分 Hive 类型，请注意检查你的类型。

下面列出 HdfsWriter 针对 Hive 数据类型转换列表:

| DataX 内部类型 | HIVE 数据类型               |
| -------------- | --------------------------- |
| Long           | TINYINT,SMALLINT,INT,BIGINT |
| Double         | FLOAT,DOUBLE                |
| String         | STRING,VARCHAR,CHAR         |
| Boolean        | BOOLEAN                     |
| Date           | DATE,TIMESTAMP              |

#### 4 配置步骤

- 步骤一、在Hive中创建数据库、表 Hive数据库在HDFS上存储配置,在hive安装目录下 conf/hive-site.xml文件中配置，默认值为：/user/hive/warehouse 如下所示：

```
<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive/warehouse</value>
    <description>location of default database for the warehouse</description>
  </property>
```

Hive建库／建表语法 参考 [Hive操作手册](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)

例： （1）建立存储为textfile文件类型的表

```
create database IF NOT EXISTS hdfswriter;
use hdfswriter;
create table text_table(
col1  TINYINT,
col2  SMALLINT,
col3  INT,
col4  BIGINT,
col5  FLOAT,
col6  DOUBLE,
col7  STRING,
col8  VARCHAR(10),
col9  CHAR(10),
col10  BOOLEAN,
col11 date,
col12 TIMESTAMP
)
row format delimited
fields terminated by "\t"
STORED AS TEXTFILE;
```

text_table在hdfs上存储路径为：/user/hive/warehouse/hdfswriter.db/text_table/

（2）建立存储为orcfile文件类型的表

```
create database IF NOT EXISTS hdfswriter;
use hdfswriter;
create table orc_table(
col1  TINYINT,
col2  SMALLINT,
col3  INT,
col4  BIGINT,
col5  FLOAT,
col6  DOUBLE,
col7  STRING,
col8  VARCHAR(10),
col9  CHAR(10),
col10  BOOLEAN,
col11 date,
col12 TIMESTAMP
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS ORC;
```

orc_table在hdfs上存储路径为：/user/hive/warehouse/hdfswriter.db/orc_table/

- 步骤二、根据步骤一的配置信息配置HdfsWriter作业
